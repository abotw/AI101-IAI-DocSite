{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI101-IAI: Introduction to Artificial Intelligence","text":"<p>Welcome to AI101, a comprehensive documentation site dedicated to exploring the world of Artificial Intelligence (AI). This project is designed as a personal and educational hub where enthusiasts, students, and professionals can learn, explore, and reference AI concepts, technologies, and applications.</p>"},{"location":"#overview","title":"Overview","text":"<p>Artificial Intelligence is a rapidly evolving field of computer science that focuses on creating systems capable of performing tasks that traditionally require human intelligence. From machine learning and natural language processing to computer vision and robotics, AI is transforming industries, research, and everyday life.</p> <p>AI101 aims to serve as a structured, accessible, and constantly evolving repository of knowledge for anyone looking to understand AI\u2014from foundational concepts to advanced topics.</p>"},{"location":"#features-of-the-documentation-site","title":"Features of the Documentation Site","text":"<ul> <li> <p>Comprehensive Coverage:     Includes tutorials, guides, explanations, and real-world applications of AI across different domains.</p> </li> <li> <p>Beginner-Friendly:     Sections designed for newcomers to build a solid understanding of AI fundamentals.</p> </li> <li> <p>Advanced Insights:     In-depth discussions on modern AI techniques, algorithms, and frameworks.</p> </li> <li> <p>Cross-Referencing:     Easy navigation between concepts, linking related topics for a seamless learning experience.</p> </li> <li> <p>Up-to-Date Resources:     Curated references to papers, articles, and open-source tools to stay current with the AI landscape.</p> </li> <li> <p>Hands-On Examples:     Code snippets, visualizations, and practical examples to help readers experiment and learn actively.</p> </li> </ul>"},{"location":"#key-topics","title":"Key Topics","text":"<p>The AI101 site covers a wide range of AI topics, including but not limited to:</p> <ul> <li> <p>Foundations of AI:     Introduction to AI, history, key concepts, problem-solving strategies.</p> </li> <li> <p>Machine Learning:     Supervised, unsupervised, and reinforcement learning, model evaluation, and practical ML workflows.</p> </li> <li> <p>Deep Learning:     Neural networks, CNNs, RNNs, Transformers, and modern architectures.</p> </li> <li> <p>Natural Language Processing (NLP):     Text processing, language models, sentiment analysis, and question answering.</p> </li> <li> <p>Computer Vision:     Image recognition, object detection, video analysis, and image generation.</p> </li> <li> <p>Robotics and Automation:     AI in robotics, autonomous systems, and control algorithms.</p> </li> <li> <p>Ethics and AI Society:     Responsible AI, fairness, bias, and societal impact.</p> </li> <li> <p>AI Tools &amp; Frameworks:     Overview of popular AI libraries such as TensorFlow, PyTorch, OpenCV, and more.</p> </li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<p>AI101 is designed as a documentation site with the following structure:</p> <pre><code>AI101/\n\u251c\u2500 docs/          # Markdown files for AI topics\n\u251c\u2500 assets/        # Images, diagrams, and examples\n\u251c\u2500 tutorials/     # Step-by-step tutorials and hands-on exercises\n\u251c\u2500 examples/      # Code snippets and demo projects\n\u251c\u2500 index.md       # Homepage / overview\n\u2514\u2500 README.md      # Project introduction\n</code></pre> <p>This structure ensures content is modular, easy to maintain, and scalable as the site grows.</p>"},{"location":"#why-ai101","title":"Why AI101?","text":"<p>AI is a complex and fast-growing field. Many learners face the challenge of fragmented information scattered across books, papers, and online resources. AI101 consolidates essential knowledge into a single, organized, and accessible site.</p> <p>Whether your goal is to learn AI from scratch, stay updated with trends, or apply AI in projects, AI101 provides the guidance, resources, and context to support your journey.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To explore AI101:</p> <ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/yourusername/AI101.git\n</code></pre> </li> <li> <p>Open <code>index.md</code> to start reading the main content.</p> </li> <li> <p>Navigate through different topics using the sidebar or table of contents.</p> </li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>AI101 is an open and evolving project. Contributions are welcome! You can:</p> <ul> <li> <p>Add new AI topics or tutorials.</p> </li> <li> <p>Improve existing content.</p> </li> <li> <p>Provide code examples or visualizations.</p> </li> <li> <p>Suggest edits or updates to ensure accuracy.</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is open-source under the MIT License. Feel free to use, modify, and share knowledge responsibly.</p> <p>AI101 is more than just a documentation site\u2014it\u2019s a curated AI learning experience. Whether you are a student, researcher, or curious explorer, AI101 aims to be your guide in navigating the exciting world of Artificial Intelligence.</p>"},{"location":"draft/","title":"Draft","text":"<p>\u4e0b\u9762\u8fd9\u4efd\u76ee\u5f55\u662f\u4e00\u4e2a\u5b8c\u6574\u7684 AI \u673a\u68b0\u81c2\u5f00\u53d1\u5b66\u4e60\u4f53\u7cfb\u7684\u8bfe\u7a0b\u84dd\u56fe\u3002\u5b83\u4ece\u786c\u4ef6\u88c5\u914d \u2192 \u7cfb\u7edf\u73af\u5883 \u2192 \u7f16\u7a0b\u63a7\u5236 \u2192 \u8ba1\u7b97\u673a\u89c6\u89c9 \u2192 AI \u611f\u77e5 \u2192 ROS \u673a\u5668\u4eba\u63a7\u5236 \u2192 \u9ad8\u7ea7\u8fd0\u52a8\u5b66\u89c4\u5212\u9010\u6b65\u8fdb\u9636\u3002</p>"},{"location":"draft/#_1","title":"\ud83e\udded \u4e00\u3001\u6574\u4f53\u5b66\u4e60\u601d\u8def\uff08\u5b66\u4e60\u8def\u7ebf\u56fe\uff09","text":"<p>\u8fd9\u4efd\u6559\u7a0b\u662f\u4e00\u4e2a\u5178\u578b\u7684 \u300c\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u5168\u6d41\u7a0b\u8bfe\u7a0b\u300d\uff0c\u5b66\u4e60\u601d\u8def\u5e94\u9075\u5faa\u4ee5\u4e0b\u9636\u6bb5\uff1a</p>"},{"location":"draft/#1","title":"\u9636\u6bb5 1\uff1a\u786c\u4ef6\u5165\u95e8\u4e0e\u7cfb\u7edf\u73af\u5883","text":"<p>\u76ee\u6807\uff1a\u80fd\u7ec4\u88c5\u3001\u8fde\u63a5\u5e76\u63a7\u5236\u673a\u68b0\u81c2\u7684\u57fa\u672c\u52a8\u4f5c\u3002</p> <ul> <li> <p>\u5b66\u4e60\u5185\u5bb9\uff1a</p> <ul> <li> <p>\u673a\u68b0\u81c2\u7684 \u5b89\u88c5\u3001\u4e2d\u4f4d\u6821\u51c6\u3001\u8235\u673a\u63a7\u5236</p> </li> <li> <p>Linux \u57fa\u7840\u547d\u4ee4\u3001Ubuntu \u73af\u5883</p> </li> <li> <p>JupyterLab \u5f00\u53d1\u73af\u5883\u642d\u5efa</p> </li> <li> <p>\u57fa\u672c\u7684 RGB \u706f\u3001\u8702\u9e23\u5668\u3001\u8235\u673a\u63a7\u5236\u5b9e\u9a8c</p> </li> </ul> </li> <li> <p>\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li> <p>\u4e32\u53e3\u901a\u4fe1 / PWM \u63a7\u5236\u8235\u673a</p> </li> <li> <p>Linux \u6587\u4ef6\u7cfb\u7edf\u4e0e\u547d\u4ee4\u64cd\u4f5c</p> </li> <li> <p>Python \u63a7\u5236\u786c\u4ef6\u7684\u57fa\u672c\u63a5\u53e3</p> </li> <li> <p>\u8c03\u8bd5\u5de5\u5177\u4e0e\u8bbe\u5907 ID \u7ed1\u5b9a</p> </li> </ul> </li> </ul>"},{"location":"draft/#2opencv","title":"\u9636\u6bb5 2\uff1aOpenCV \u56fe\u50cf\u5904\u7406\u57fa\u7840","text":"<p>\u76ee\u6807\uff1a\u638c\u63e1\u56fe\u50cf\u7684\u8bfb\u53d6\u3001\u5904\u7406\u3001\u53d8\u6362\u4e0e\u7ed8\u5236\uff0c\u4e3a AI \u89c6\u89c9\u5960\u5b9a\u57fa\u7840\u3002</p> <ul> <li> <p>\u5b66\u4e60\u5185\u5bb9\uff1a</p> <ul> <li> <p>\u56fe\u50cf I/O\uff08\u8bfb\u5199\u3001\u663e\u793a\u3001\u4fdd\u5b58\uff09</p> </li> <li> <p>\u56fe\u50cf\u53d8\u6362\uff08\u7f29\u653e\u3001\u65cb\u8f6c\u3001\u5e73\u79fb\u3001\u955c\u50cf\uff09</p> </li> <li> <p>\u56fe\u50cf\u589e\u5f3a\uff08\u7070\u5ea6\u5316\u3001\u4e8c\u503c\u5316\u3001\u8fb9\u7f18\u68c0\u6d4b\uff09</p> </li> <li> <p>\u56fe\u5f62\u7ed8\u5236\uff08\u7ebf\u6bb5\u3001\u77e9\u5f62\u3001\u6587\u5b57\uff09</p> </li> </ul> </li> <li> <p>\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li> <p>OpenCV \u7684\u6838\u5fc3\u51fd\u6570\u4e0e\u77e9\u9635\u64cd\u4f5c</p> </li> <li> <p>\u56fe\u50cf\u5750\u6807\u4e0e\u50cf\u7d20\u8bbf\u95ee</p> </li> <li> <p>\u56fe\u50cf\u53d8\u6362\uff08\u4eff\u5c04 / \u900f\u89c6\uff09</p> </li> <li> <p>\u57fa\u4e8e\u9608\u503c\u548c\u8fb9\u7f18\u7684\u76ee\u6807\u8bc6\u522b\u57fa\u7840</p> </li> </ul> </li> </ul>"},{"location":"draft/#3ai","title":"\u9636\u6bb5 3\uff1aAI \u89c6\u89c9\u4e0e\u8bc6\u522b","text":"<p>\u76ee\u6807\uff1a\u5b66\u4f1a\u8ba9\u673a\u5668\u4eba\u300c\u770b\u61c2\u4e16\u754c\u300d\u3002</p> <ul> <li> <p>\u5b66\u4e60\u5185\u5bb9\uff1a</p> <ul> <li> <p>\u989c\u8272\u8bc6\u522b\u3001\u624b\u52bf\u8bc6\u522b\u3001\u4eba\u8138\u8bc6\u522b</p> </li> <li> <p>\u6a21\u578b\u8bad\u7ec3\u4e0e YOLO \u7269\u4f53\u68c0\u6d4b</p> </li> <li> <p>\u5783\u573e\u8bc6\u522b\u3001\u76ee\u6807\u8bc6\u522b\u7b49\u5e94\u7528</p> </li> </ul> </li> <li> <p>\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li> <p>\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u4e0e\u5206\u7c7b</p> </li> <li> <p>\u989c\u8272\u7a7a\u95f4\u8f6c\u6362\u4e0e\u9608\u503c\u5206\u5272\uff08HSV\u3001HLS\uff09</p> </li> <li> <p>\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\uff08YOLO \u7cfb\u5217\uff09</p> </li> <li> <p>\u5b9e\u65f6\u89c6\u9891\u6d41\u68c0\u6d4b\u4e0e\u8ffd\u8e2a\u7b97\u6cd5\uff08\u5982\u5149\u6d41\uff09</p> </li> </ul> </li> </ul>"},{"location":"draft/#4ai","title":"\u9636\u6bb5 4\uff1aAI \u89c6\u89c9\u8ffd\u8e2a\u4e0e\u6293\u53d6","text":"<p>\u76ee\u6807\uff1a\u5b9e\u73b0\u300c\u770b \u2192 \u60f3 \u2192 \u52a8\u300d\u7684\u95ed\u73af\u63a7\u5236\u3002</p> <ul> <li> <p>\u5b66\u4e60\u5185\u5bb9\uff1a</p> <ul> <li> <p>\u8272\u5757\u3001\u4eba\u8138\u7684\u68c0\u6d4b\u4e0e\u8ffd\u8e2a</p> </li> <li> <p>PID \u63a7\u5236\u7b97\u6cd5\u5b9e\u73b0\u81ea\u52a8\u8ffd\u8e2a</p> </li> <li> <p>\u624b\u52bf\u8bc6\u522b\u89e6\u53d1\u673a\u68b0\u81c2\u52a8\u4f5c</p> </li> <li> <p>\u989c\u8272\u8bc6\u522b\u4e0e\u5206\u62e3\u3001\u79ef\u6728\u6293\u53d6\u4efb\u52a1</p> </li> </ul> </li> <li> <p>\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li> <p>\u56fe\u50cf\u5750\u6807 \u2192 \u673a\u68b0\u81c2\u5750\u6807\u6620\u5c04</p> </li> <li> <p>PID \u63a7\u5236\u5b9e\u73b0\u5e73\u6ed1\u8ddf\u8e2a</p> </li> <li> <p>\u76ee\u6807\u8bc6\u522b + \u52a8\u4f5c\u63a7\u5236\u8054\u52a8\u903b\u8f91</p> </li> <li> <p>\u591a\u76ee\u6807\u51b3\u7b56\uff08\u989c\u8272\u3001\u624b\u52bf\u3001\u4f4d\u7f6e\uff09</p> </li> </ul> </li> </ul>"},{"location":"draft/#5","title":"\u9636\u6bb5 5\uff1a\u8bed\u97f3\u63a7\u5236\u4e0e\u4eba\u673a\u4ea4\u4e92","text":"<p>\u76ee\u6807\uff1a\u8ba9\u673a\u5668\u4eba\u5177\u5907\u8bed\u97f3\u7406\u89e3\u4e0e\u54cd\u5e94\u80fd\u529b\u3002</p> <ul> <li> <p>\u5b66\u4e60\u5185\u5bb9\uff1a</p> <ul> <li> <p>\u8bed\u97f3\u8bc6\u522b\u4e0e\u64ad\u62a5\u6a21\u5757</p> </li> <li> <p>\u8bed\u97f3\u89e6\u53d1\u63a7\u5236\u3001\u8bed\u97f3\u64ad\u62a5\u7ed3\u679c</p> </li> <li> <p>\u5c06\u8bed\u97f3\u4e0e\u89c6\u89c9\u4efb\u52a1\u7ed3\u5408\uff08\u989c\u8272\u8bc6\u522b + \u64ad\u62a5\uff09</p> </li> </ul> </li> <li> <p>\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li> <p>\u8bed\u97f3\u8bc6\u522b\u6a21\u5757\uff08\u79bb\u7ebf/\u5728\u7ebf\uff09</p> </li> <li> <p>\u8bed\u97f3\u89e6\u53d1\u4e0e\u72b6\u6001\u673a\u903b\u8f91</p> </li> <li> <p>\u591a\u6a21\u6001\u878d\u5408\uff08\u8bed\u97f3 + \u89c6\u89c9 + \u52a8\u4f5c\uff09</p> </li> <li> <p>\u4e2d\u6587 / \u82f1\u6587 TTS\uff08\u6587\u672c\u8f6c\u8bed\u97f3\uff09</p> </li> </ul> </li> </ul>"},{"location":"draft/#6ros","title":"\u9636\u6bb5 6\uff1aROS \u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf","text":"<p>\u76ee\u6807\uff1a\u5b66\u4e60\u6807\u51c6\u7684\u673a\u5668\u4eba\u8f6f\u4ef6\u67b6\u6784\uff0c\u638c\u63e1 ROS \u901a\u4fe1\u673a\u5236\u3002</p> <ul> <li> <p>\u5b66\u4e60\u5185\u5bb9\uff1a</p> <ul> <li> <p>ROS \u57fa\u7840\u547d\u4ee4\u3001Publisher / Subscriber</p> </li> <li> <p>\u81ea\u5b9a\u4e49\u6d88\u606f\u4e0e\u670d\u52a1</p> </li> <li> <p>TF \u5750\u6807\u53d8\u6362</p> </li> </ul> </li> <li> <p>\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li> <p>ROS \u8282\u70b9\u3001\u8bdd\u9898\u3001\u670d\u52a1\u3001\u52a8\u4f5c\u673a\u5236</p> </li> <li> <p>TF \u6846\u67b6\u7684\u7a7a\u95f4\u5b9a\u4f4d\u4e0e\u5750\u6807\u7cfb\u8f6c\u6362</p> </li> <li> <p>Python ROS API \u4f7f\u7528</p> </li> </ul> </li> </ul>"},{"location":"draft/#7ros-opencv","title":"\u9636\u6bb5 7\uff1aROS + OpenCV \u7efc\u5408\u5e94\u7528","text":"<p>\u76ee\u6807\uff1a\u8ba9\u673a\u5668\u4eba\u5728 ROS \u6846\u67b6\u4e0b\u8fd0\u884c\u89c6\u89c9\u4efb\u52a1\u3002</p> <ul> <li> <p>\u5b66\u4e60\u5185\u5bb9\uff1a</p> <ul> <li> <p>\u76ee\u6807\u68c0\u6d4b\u3001\u4eba\u8138\u8bc6\u522b\u3001\u59ff\u6001\u4f30\u8ba1</p> </li> <li> <p>\u5149\u6d41\u7b97\u6cd5\u3001\u7279\u5f81\u70b9\u8ffd\u8e2a\u3001Hough \u68c0\u6d4b</p> </li> <li> <p>QR \u4e8c\u7ef4\u7801\u3001AR \u89c6\u89c9\u7b49</p> </li> </ul> </li> <li> <p>\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li> <p>ROS \u56fe\u50cf\u4f20\u8f93\uff08<code>cv_bridge</code>\uff09</p> </li> <li> <p>OpenCV \u7b97\u6cd5\u4e0e ROS topic \u96c6\u6210</p> </li> <li> <p>\u591a\u7b97\u6cd5\u878d\u5408\uff08\u68c0\u6d4b + \u8ffd\u8e2a + \u5206\u5272\uff09</p> </li> </ul> </li> </ul>"},{"location":"draft/#8mediapipe-ai","title":"\u9636\u6bb5 8\uff1aMediaPipe \u9ad8\u7ea7 AI \u611f\u77e5","text":"<p>\u76ee\u6807\uff1a\u5b9e\u73b0\u57fa\u4e8e MediaPipe \u7684\u5b9e\u65f6\u59ff\u6001\u4e0e\u624b\u52bf\u63a7\u5236\u3002</p> <ul> <li> <p>\u5b66\u4e60\u5185\u5bb9\uff1a</p> <ul> <li> <p>\u624b\u90e8\u68c0\u6d4b\u3001\u59ff\u6001\u68c0\u6d4b\u3001\u6574\u4f53\u68c0\u6d4b</p> </li> <li> <p>\u4eba\u8138\u8bc6\u522b\u4e0e\u7279\u6548\u3001\u4e09\u7ef4\u7269\u4f53\u8bc6\u522b</p> </li> <li> <p>\u624b\u52bf\u8bc6\u522b\u63a7\u5236\u673a\u68b0\u81c2</p> </li> </ul> </li> <li> <p>\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li> <p>MediaPipe Graph \u7ba1\u7ebf\u673a\u5236</p> </li> <li> <p>\u624b\u90e8\u4e0e\u4eba\u4f53\u5173\u952e\u70b9\u63d0\u53d6\uff0821/33 \u70b9\uff09</p> </li> <li> <p>\u624b\u52bf\u6620\u5c04\u5230\u673a\u68b0\u81c2\u63a7\u5236</p> </li> <li> <p>\u591a\u6a21\u6001\u878d\u5408\uff08\u6444\u50cf\u5934\u8f93\u5165 \u2192 \u63a7\u5236\u8f93\u51fa\uff09</p> </li> </ul> </li> </ul>"},{"location":"draft/#9","title":"\u9636\u6bb5 9\uff1a\u8fd0\u52a8\u5b66\u4e0e\u8def\u5f84\u89c4\u5212","text":"<p>\u76ee\u6807\uff1a\u5b66\u4e60\u673a\u5668\u4eba\u771f\u5b9e\u8fd0\u52a8\u63a7\u5236\u4e0e\u89c4\u5212\u7b97\u6cd5\u3002</p> <ul> <li> <p>\u5b66\u4e60\u5185\u5bb9\uff1a</p> <ul> <li> <p>MoveIt \u73af\u5883\u914d\u7f6e\u4e0e\u4eff\u771f</p> </li> <li> <p>\u7b1b\u5361\u5c14\u8def\u5f84\u89c4\u5212\u3001\u78b0\u649e\u68c0\u6d4b</p> </li> <li> <p>\u968f\u673a\u79fb\u52a8\u4e0e\u8f68\u8ff9\u8bbe\u8ba1</p> </li> </ul> </li> <li> <p>\u6280\u672f\u8981\u70b9\uff1a</p> <ul> <li> <p>\u673a\u68b0\u81c2\u8fd0\u52a8\u5b66\uff08\u6b63\u89e3 / \u9006\u89e3\uff09</p> </li> <li> <p>MoveIt! + ROS \u7684\u63a7\u5236\u63a5\u53e3</p> </li> <li> <p>\u89c4\u5212\u7b97\u6cd5\u4e0e\u7ea6\u675f\u63a7\u5236</p> </li> <li> <p>\u673a\u5668\u4eba\u771f\u5b9e\u673a\u63a7\u5236\u4e0e\u907f\u969c</p> </li> </ul> </li> </ul>"},{"location":"draft/#_2","title":"\ud83e\udde0 \u4e8c\u3001\u5173\u952e\u6280\u672f\u8981\u70b9\uff08\u6838\u5fc3\u77e5\u8bc6\u5730\u56fe\uff09","text":"\u9886\u57df \u6838\u5fc3\u6280\u672f \u5b66\u4e60\u6210\u679c \u786c\u4ef6\u63a7\u5236 \u8235\u673a\u63a7\u5236\u3001PWM\u3001\u4e32\u53e3\u901a\u4fe1 \u63a7\u5236\u673a\u68b0\u81c2\u52a8\u4f5c \u7cfb\u7edf\u73af\u5883 Linux \u547d\u4ee4\u3001JupyterLab\u3001\u8bbe\u5907\u7ed1\u5b9a \u5b8c\u5584\u7684\u5f00\u53d1\u73af\u5883 \u56fe\u50cf\u5904\u7406 OpenCV \u56fe\u50cf\u53d8\u6362\u3001\u6ee4\u6ce2\u3001\u7ed8\u5236 \u80fd\u5b9e\u73b0\u56fe\u50cf\u5206\u6790 AI\u89c6\u89c9 YOLO / MediaPipe / \u7279\u5f81\u8bc6\u522b \u80fd\u8bc6\u522b\u989c\u8272\u3001\u624b\u52bf\u3001\u4eba\u8138 \u63a7\u5236\u7b97\u6cd5 PID\u3001\u5750\u6807\u6620\u5c04 \u5b9e\u73b0\u81ea\u52a8\u8ffd\u8e2a\u4e0e\u6293\u53d6 \u8bed\u97f3\u6a21\u5757 ASR\uff08\u8bed\u97f3\u8bc6\u522b\uff09\u3001TTS\uff08\u8bed\u97f3\u5408\u6210\uff09 \u8bed\u97f3\u4ea4\u4e92\u4e0e\u63a7\u5236 ROS \u6846\u67b6 Publisher / Subscriber / TF \u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf \u8fd0\u52a8\u5b66 MoveIt!\u3001\u8def\u5f84\u89c4\u5212\u3001\u9006\u8fd0\u52a8\u5b66 \u9ad8\u7ea7\u673a\u68b0\u81c2\u63a7\u5236"},{"location":"draft/#_3","title":"\ud83e\udde9 \u4e09\u3001\u5b66\u4e60\u5efa\u8bae\u4e0e\u987a\u5e8f","text":"<p>1\ufe0f\u20e3 \u6253\u597d\u57fa\u7840 \u5148\u5b8c\u6210\u673a\u68b0\u81c2\u7684\u7ec4\u88c5\u3001\u4e2d\u4f4d\u8c03\u8bd5\u3001Linux \u57fa\u7840\u64cd\u4f5c\u3002 \u27a1\ufe0f \u5efa\u8bae\u9636\u6bb5\u76ee\u6807\uff1a\u80fd\u901a\u8fc7 Python \u63a7\u5236\u8235\u673a\u8fd0\u52a8\u3002</p> <p>2\ufe0f\u20e3 \u5b66\u4e60\u56fe\u50cf\u4e0eAI\u89c6\u89c9 \u4ece OpenCV \u57fa\u7840\u5b66\u8d77\uff0c\u518d\u8fdb\u5165\u989c\u8272\u8bc6\u522b\u3001\u624b\u52bf\u8bc6\u522b\u3002 \u27a1\ufe0f \u9636\u6bb5\u76ee\u6807\uff1a\u80fd\u8ba9\u673a\u68b0\u81c2\u6839\u636e\u6444\u50cf\u5934\u8bc6\u522b\u989c\u8272\u3002</p> <p>3\ufe0f\u20e3 \u878d\u5408\u63a7\u5236\u903b\u8f91 \u5b66\u4e60 PID + \u89c6\u89c9\u8ffd\u8e2a + \u6293\u53d6\u7b97\u6cd5\uff0c\u5b9e\u73b0\u81ea\u4e3b\u53cd\u5e94\u3002 \u27a1\ufe0f \u9636\u6bb5\u76ee\u6807\uff1a\u80fd\u81ea\u52a8\u8bc6\u522b\u5e76\u6293\u53d6\u7269\u4f53\u3002</p> <p>4\ufe0f\u20e3 \u52a0\u5165\u8bed\u97f3\u63a7\u5236\u4e0e\u4ea4\u4e92 \u27a1\ufe0f \u9636\u6bb5\u76ee\u6807\uff1a\u80fd\u542c\u6307\u4ee4 \u2192 \u6267\u884c\u52a8\u4f5c \u2192 \u64ad\u62a5\u7ed3\u679c\u3002</p> <p>5\ufe0f\u20e3 \u8fdb\u9636 ROS \u4e0e MoveIt! \u5b66\u4e60 ROS \u8282\u70b9\u901a\u8baf\u3001MoveIt \u8fd0\u52a8\u89c4\u5212\uff0c\u5b9e\u73b0\u9ad8\u9636\u63a7\u5236\u3002 \u27a1\ufe0f \u9636\u6bb5\u76ee\u6807\uff1a\u5b9e\u73b0\u4eff\u771f + \u771f\u673a\u8f68\u8ff9\u89c4\u5212\u3002</p> <p>6\ufe0f\u20e3 \u9ad8\u7ea7\u611f\u77e5\u4e0eAI\u63a7\u5236\uff08MediaPipe\uff09 \u6574\u5408\u89c6\u89c9\u3001\u8bed\u97f3\u3001\u52a8\u4f5c\u63a7\u5236\uff0c\u6253\u9020\u667a\u80fd\u673a\u68b0\u81c2\u3002 \u27a1\ufe0f \u9636\u6bb5\u76ee\u6807\uff1a\u7528\u624b\u52bf\u63a7\u5236\u673a\u68b0\u81c2\u505a\u590d\u6742\u4efb\u52a1\u3002</p>"},{"location":"draft/#_4","title":"\ud83c\udfc1 \u56db\u3001\u5b66\u4e60\u7ec8\u70b9\u4e0e\u6210\u679c","text":"<p>\u5b8c\u6210\u6574\u4e2a\u6559\u7a0b\u540e\uff0c\u4f60\u5c06\u5177\u5907\uff1a</p> <ul> <li> <p>\u72ec\u7acb\u642d\u5efa Linux + Python + ROS \u5f00\u53d1\u73af\u5883\u7684\u80fd\u529b\uff1b</p> </li> <li> <p>\u719f\u6089 OpenCV \u4e0e MediaPipe \u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u80fd\uff1b</p> </li> <li> <p>\u638c\u63e1 AI \u8bc6\u522b\u3001PID \u63a7\u5236\u3001\u8bed\u97f3\u4ea4\u4e92\u3001\u8def\u5f84\u89c4\u5212\uff1b</p> </li> <li> <p>\u80fd\u5f00\u53d1\u5b8c\u6574\u7684\u300cAI\u89c6\u89c9 + \u63a7\u5236 + \u8bed\u97f3 + ROS\u300d\u673a\u5668\u4eba\u7cfb\u7edf\u3002</p> </li> </ul> <p>\u8fd9\u4efd\u6559\u7a0b\u80fd\u8ba9\u4f60\u4ece\u201c\u4f1a\u7528\u673a\u68b0\u81c2\u201d\u6210\u957f\u4e3a\u201c\u80fd\u5f00\u53d1\u667a\u80fd\u673a\u5668\u4eba\u7cfb\u7edf\u201d\u7684\u5de5\u7a0b\u5e08\u3002</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:roi","title":"ROI","text":"<ul> <li>            ROI          </li> </ul>"},{"location":"tags/#tag:ai","title":"ai","text":"<ul> <li>            OpenCV Common Usages          </li> </ul>"},{"location":"tags/#tag:computer-vision","title":"computer-vision","text":"<ul> <li>            Introduction to OpenCV          </li> </ul>"},{"location":"tags/#tag:cs101","title":"cs101","text":"<ul> <li>            Introduction to OpenCV          </li> </ul>"},{"location":"tags/#tag:image","title":"image","text":"<ul> <li>            Image Operations in OpenCV          </li> <li>            \ud83d\udcf8 Reading an Image with `cv2.imread()`          </li> </ul>"},{"location":"tags/#tag:image-processing","title":"image-processing","text":"<ul> <li>            Introduction to OpenCV          </li> <li>            OpenCV Common Usages          </li> </ul>"},{"location":"tags/#tag:opencv","title":"opencv","text":"<ul> <li>            Introduction to OpenCV          </li> <li>            OpenCV Common Usages          </li> <li>            Read and Display a Video File          </li> <li>            \ud83d\udcf8 Reading an Image with `cv2.imread()`          </li> </ul>"},{"location":"tags/#tag:python","title":"python","text":"<ul> <li>            Introduction to OpenCV          </li> <li>            OpenCV Common Usages          </li> </ul>"},{"location":"tags/#tag:read","title":"read","text":"<ul> <li>            \ud83d\udcf8 Reading an Image with `cv2.imread()`          </li> </ul>"},{"location":"tags/#tag:tutorial","title":"tutorial","text":"<ul> <li>            Read and Display a Video File          </li> </ul>"},{"location":"tags/#tag:video","title":"video","text":"<ul> <li>            Read and Display a Video File          </li> </ul>"},{"location":"Dofbot-SE/01rgb/","title":"\ud83c\udf08 Control the RGB Light on Dofbot","text":""},{"location":"Dofbot-SE/01rgb/#1-overview","title":"1. Overview","text":"<p>The Dofbot expansion board supports controlling servos, an RGB LED, and a buzzer. The low-level drivers are provided as a Python library called Arm_Lib, which can be installed from the <code>Dofbot.zip</code> package.</p> <p>After extraction and installation:</p> <pre><code>unzip Dofbot.zip\ncd Dofbot/0.py_install &amp;&amp; sudo python3 setup.py install\n</code></pre> <p>If you see output like:</p> <pre><code>Arm_Lib = x.x.x installed successfully\n</code></pre> <p>then the library is ready to use.</p>"},{"location":"Dofbot-SE/01rgb/#2-api-reference","title":"2. API Reference","text":"<p>Function:</p> <pre><code>Arm.Arm_RGB_set(R, G, B)\n</code></pre> <p>Description: Sets the RGB LED color on the Dofbot expansion board.</p> <p>Parameters:</p> Parameter Type Range Description <code>R</code> int 0\u2013255 Red brightness <code>G</code> int 0\u2013255 Green brightness <code>B</code> int 0\u2013255 Blue brightness <p>Return: None</p>"},{"location":"Dofbot-SE/01rgb/#3-example-code","title":"3. Example Code","text":"<p>File Path: <code>/home/yahboom/Dofbot/3.ctrl_Arm/1.rgb.ipynb</code></p> <p>This script continuously cycles the RGB LED through red, green, and blue colors at 0.5-second intervals.</p> <pre><code>#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n----------------------------------------------------------\nControl RGB LED on Dofbot Expansion Board\n----------------------------------------------------------\nThis script cycles the onboard RGB LED through red, green,\nand blue colors in a loop. Press Ctrl+C to stop.\n----------------------------------------------------------\n\"\"\"\n\nimport time\nfrom Arm_Lib import Arm_Device\n\n# Initialize the robotic arm device\narm = Arm_Device()\ntime.sleep(0.1)  # Short delay for initialization\n\ndef cycle_rgb_lights():\n    \"\"\"Continuously cycle the RGB LED through red, green, and blue.\"\"\"\n    while True:\n        arm.Arm_RGB_set(50, 0, 0)   # Red\n        time.sleep(0.5)\n        arm.Arm_RGB_set(0, 50, 0)   # Green\n        time.sleep(0.5)\n        arm.Arm_RGB_set(0, 0, 50)   # Blue\n        time.sleep(0.5)\n        print(\"RGB cycle complete.\")\n\ntry:\n    cycle_rgb_lights()\nexcept KeyboardInterrupt:\n    # Release the Arm device safely on exit\n    del arm\n    print(\"Program terminated by user.\")\n</code></pre>"},{"location":"Dofbot-SE/01rgb/#4-running-the-code-in-jupyterlab","title":"4. Running the Code in JupyterLab","text":"<ol> <li> <p>Open JupyterLab.</p> </li> <li> <p>Navigate to <code>/home/yahboom/Dofbot/3.ctrl_Arm/1.rgb.ipynb</code>.</p> </li> <li> <p>Click Run All Cells in the toolbar.</p> </li> <li> <p>Observe the RGB LED on the Dofbot board cycling every 0.5 seconds:     \ud83d\udd34 \u2192 \ud83d\udfe2 \u2192 \ud83d\udd35 \u2192 (repeat)</p> </li> </ol> <p>To stop the program, click the Stop (\u25a0) button in JupyterLab\u2019s toolbar.</p>"},{"location":"Dofbot-SE/01rgb/#5-improvements-in-this-version","title":"5. Improvements in This Version","text":"Area Original Improved Indentation Misaligned <code>while True:</code> loop Proper indentation Naming <code>Arm</code> Changed to lowercase <code>arm</code> for Python convention Function design Inline infinite loop Wrapped into <code>cycle_rgb_lights()</code> Comments Sparse Added clear, structured comments Docstring None Added a full header docstring Exception handling Present but minimal Clear user feedback and safe cleanup Print statements \u201cEND OF LINE!\u201d Replaced with meaningful output"},{"location":"Dofbot-SE/01rgb/#6-learning-notes","title":"6. Learning Notes","text":"<ul> <li> <p>You can adjust the brightness or mix colors by changing RGB values.     Example:</p> <pre><code>arm.Arm_RGB_set(30, 30, 0)  # Yellow (Red + Green)\n</code></pre> </li> <li> <p>To turn off the LED:</p> <pre><code>arm.Arm_RGB_set(0, 0, 0)\n</code></pre> </li> <li> <p>The color update delay can be customized by changing the <code>time.sleep()</code> interval.</p> </li> </ul>"},{"location":"Dofbot-SE/01rgb/#summary","title":"\u2705 Summary","text":"<p>This improved version follows Pythonic style, improves readability, and is more maintainable. It can serve as a template for other Dofbot control scripts \u2014 such as controlling servos or the buzzer.</p>"},{"location":"Dofbot-SE/02buzzer/","title":"6.2 Buzzer Control","text":""},{"location":"Dofbot-SE/02buzzer/#1-api-overview","title":"1. API Overview","text":"<p>The buzzer on the robotic arm expansion board can be controlled using the following APIs:</p>"},{"location":"Dofbot-SE/02buzzer/#arm_buzzer_ondelay255","title":"<code>Arm_Buzzer_On(delay=255)</code>","text":"<p>Function: Activates (turns on) the buzzer.</p> <p>Parameters:</p> <ul> <li> <p><code>delay</code> (int, optional) \u2014 Defines how long the buzzer should sound.</p> <ul> <li> <p>Range: 1\u201350</p> </li> <li> <p>Each unit represents 100 milliseconds.</p> </li> <li> <p>For example:</p> <ul> <li> <p><code>delay=1</code> \u2192 100 ms</p> </li> <li> <p><code>delay=2</code> \u2192 200 ms</p> </li> <li> <p><code>delay=50</code> \u2192 5 seconds (maximum)</p> </li> </ul> </li> <li> <p>If <code>delay</code> is not provided or set to 255, the buzzer will keep buzzing continuously until it is manually turned off.</p> </li> </ul> </li> </ul> <p>Return value: None</p>"},{"location":"Dofbot-SE/02buzzer/#arm_buzzer_off","title":"<code>Arm_Buzzer_Off()</code>","text":"<p>Function: Turns off the buzzer immediately.</p> <p>Parameters: None</p> <p>Return value: None</p>"},{"location":"Dofbot-SE/02buzzer/#2-example-code","title":"2. Example Code","text":"<p>File path: <code>/home/yahboom/Dofbot/3.ctrl_Arm/2.beep.ipynb</code></p> <pre><code>#!/usr/bin/env python3\n# coding=utf-8\nimport time\nfrom Arm_Lib import Arm_Device\n\n# Create an Arm_Device object\nArm = Arm_Device()\ntime.sleep(0.1)\n\n# The buzzer will sound automatically for 100 ms, then stop\nb_time = 1\nArm.Arm_Buzzer_On(b_time)\ntime.sleep(1)\n\n# The buzzer will sound for 300 ms, then stop\nb_time = 3\nArm.Arm_Buzzer_On(b_time)\ntime.sleep(1)\n\n# The buzzer will sound continuously\nArm.Arm_Buzzer_On()\ntime.sleep(1)\n\n# Turn off the buzzer manually\nArm.Arm_Buzzer_Off()\ntime.sleep(1)\n\n# Release the Arm object\ndel Arm\n</code></pre>"},{"location":"Dofbot-SE/02buzzer/#3-how-to-run-in-jupyterlab","title":"3. How to Run in JupyterLab","text":"<ol> <li> <p>Open JupyterLab and navigate to the file:</p> <pre><code>/home/yahboom/Dofbot/3.ctrl_Arm/2.beep.ipynb\n</code></pre> </li> <li> <p>Click the \u201cRun All\u201d button on the toolbar to execute the entire notebook.</p> </li> <li> <p>You will hear the buzzer beep three times:</p> <ul> <li> <p>The first beep lasts about 100 ms.</p> </li> <li> <p>The second beep lasts longer (300 ms).</p> </li> <li> <p>The third beep continues until manually turned off.</p> </li> </ul> </li> <li> <p>Once execution finishes, the notebook will exit automatically.</p> </li> </ol>"},{"location":"Dofbot-SE/02buzzer/#4-notes","title":"4. Notes","text":"<ul> <li> <p>The buzzer delay time is automatically handled by the API, so there is no need to add manual timing for automatic stop.</p> </li> <li> <p>Always call <code>Arm_Buzzer_Off()</code> after continuous mode to avoid prolonged buzzing.</p> </li> <li> <p>Use <code>del Arm</code> at the end of the program to properly release device resources.</p> </li> </ul>"},{"location":"Dofbot-SE/03single_servo/","title":"Control a Single Servo Motor","text":""},{"location":"Dofbot-SE/03single_servo/#1-api-overview","title":"1. API Overview","text":"<p>The robotic arm\u2019s single bus servo motor can be controlled using the following API:</p>"},{"location":"Dofbot-SE/03single_servo/#arm_serial_servo_writeid-angle-time","title":"<code>Arm_serial_servo_write(id, angle, time)</code>","text":"<p>Function: Controls a specific servo motor to rotate to a given angle.</p> <p>Parameters:</p> <ul> <li> <p><code>id</code> (int) \u2014 The ID number of the servo motor to control.</p> <ul> <li> <p>Range: 1\u20136</p> </li> <li> <p>Each ID corresponds to one servo motor:</p> <ul> <li> <p>ID 1 = bottom servo</p> </li> <li> <p>ID 6 = top servo</p> </li> </ul> </li> </ul> </li> <li> <p><code>angle</code> (int) \u2014 The target angle for the servo motor.</p> <ul> <li> <p>For most servos (ID 1\u20136, except ID 5): valid range is 0\u2013180\u00b0</p> </li> <li> <p>For servo ID 5: valid range is 0\u2013270\u00b0</p> </li> </ul> </li> <li> <p><code>time</code> (int) \u2014 The duration of movement in milliseconds.</p> <ul> <li> <p>Smaller values \u2192 faster movement.</p> </li> <li> <p>If <code>time = 0</code>, the servo moves at maximum speed.</p> </li> </ul> </li> </ul> <p>Return value: None</p>"},{"location":"Dofbot-SE/03single_servo/#2-example-code","title":"2. Example Code","text":"<p>File path: <code>/home/yahboom/Dofbot/3.ctrl_Arm/3.ctrl_servo.ipynb</code></p> <pre><code>#!/usr/bin/env python3\n# coding=utf-8\n\nimport time\nfrom Arm_Lib import Arm_Device\n\n# Create the Arm device object\nArm = Arm_Device()\ntime.sleep(0.1)\n\n# Example 1: Move a single servo (ID = 6) to 90 degrees within 500 ms\nservo_id = 6\nArm.Arm_serial_servo_write(servo_id, 90, 500)\ntime.sleep(1)\n\n# Example 2: Continuously move the same servo between different angles\ndef main():\n    while True:\n        Arm.Arm_serial_servo_write(servo_id, 120, 500)\n        time.sleep(1)\n        Arm.Arm_serial_servo_write(servo_id, 50, 500)\n        time.sleep(1)\n        Arm.Arm_serial_servo_write(servo_id, 120, 500)\n        time.sleep(1)\n        Arm.Arm_serial_servo_write(servo_id, 180, 500)\n        time.sleep(1)\n\ntry:\n    main()\nexcept KeyboardInterrupt:\n    print(\"Program closed by user.\")\n\n# Release the Arm device resource\ndel Arm\n</code></pre>"},{"location":"Dofbot-SE/03single_servo/#3-how-to-run-in-jupyterlab","title":"3. How to Run in JupyterLab","text":"<ol> <li> <p>Open JupyterLab and navigate to:</p> <pre><code>/home/yahboom/Dofbot/3.ctrl_Arm/3.ctrl_servo.ipynb\n</code></pre> </li> <li> <p>Click \u201cRun All\u201d in the toolbar to execute the entire notebook.</p> </li> <li> <p>You will see the claw (top servo, ID=6) continuously changing its angle.</p> <ul> <li>It moves between 50\u00b0, 120\u00b0, and 180\u00b0 repeatedly.</li> </ul> </li> <li> <p>To stop the program, click the \u201cStop\u201d button in the JupyterLab toolbar.</p> </li> </ol>"},{"location":"Dofbot-SE/03single_servo/#4-notes","title":"4. Notes","text":"<ul> <li> <p>The movement duration (<code>time</code>) affects speed: shorter times make the servo rotate faster.</p> </li> <li> <p>Use reasonable <code>angle</code> and <code>time</code> values to prevent excessive torque or vibration.</p> </li> <li> <p>Always use <code>del Arm</code> after your program finishes to safely release the hardware interface.</p> </li> <li> <p>If the servo doesn\u2019t move:</p> <ul> <li> <p>Check the power supply and connections.</p> </li> <li> <p>Make sure the servo ID is within the correct range (1\u20136).</p> </li> </ul> </li> </ul>"},{"location":"Dofbot-SE/04servo_position/","title":"6.4 Reading the Current Servo Position","text":""},{"location":"Dofbot-SE/04servo_position/#1-api-overview","title":"1. API Overview","text":"<p>You can read the current angle (position) of a single bus servo motor using the following API:</p>"},{"location":"Dofbot-SE/04servo_position/#arm_serial_servo_readid","title":"<code>Arm_serial_servo_read(id)</code>","text":"<p>Function: Reads the current angle value of the servo motor with the specified ID.</p> <p>Parameters:</p> <ul> <li> <p><code>id</code> (int) \u2014 The ID number of the servo motor to read.</p> <ul> <li> <p>Range: 1\u20136</p> </li> <li> <p>Each ID corresponds to one servo motor, numbered from bottom (1) to top (6).</p> </li> </ul> </li> </ul> <p>Return value:</p> <ul> <li> <p>Returns the current angle of the specified servo motor.</p> </li> <li> <p>For servo ID 5, the valid range is 0\u2013270\u00b0.</p> </li> <li> <p>For all other servos, the range is 0\u2013180\u00b0.</p> </li> </ul>"},{"location":"Dofbot-SE/04servo_position/#2-example-code","title":"2. Example Code","text":"<p>File path: <code>/home/yahboom/Dofbot/3.ctrl_Arm/4.ctrl_servo.ipynb</code></p> <pre><code>#!/usr/bin/env python3\n# coding=utf-8\n\nimport time\nfrom Arm_Lib import Arm_Device\n\n# Create the Arm device object\nArm = Arm_Device()\ntime.sleep(0.1)\n\n# Example 1: Continuously read and print the angles of all six servos\ndef main():\n    while True:\n        for i in range(6):\n            angle_value = Arm.Arm_serial_servo_read(i + 1)\n            print(f\"Servo {i + 1} angle: {angle_value}\")\n            time.sleep(0.01)\n        print(\"End of line!\\n\")\n        time.sleep(0.5)\n\ntry:\n    main()\nexcept KeyboardInterrupt:\n    print(\"Program closed by user.\")\n    pass\n\n# Example 2: Move one servo and then read its angle\nservo_id = 6\ntarget_angle = 150\nArm.Arm_serial_servo_write(servo_id, target_angle, 500)\ntime.sleep(1)\n\ncurrent_angle = Arm.Arm_serial_servo_read(servo_id)\nprint(f\"Servo {servo_id} current angle: {current_angle}\")\n\ntime.sleep(0.5)\n\n# Release the Arm device resource\ndel Arm\n</code></pre>"},{"location":"Dofbot-SE/04servo_position/#3-how-to-run-in-jupyterlab","title":"3. How to Run in JupyterLab","text":"<ol> <li> <p>Open JupyterLab and navigate to:</p> <pre><code>/home/yahboom/Dofbot/3.ctrl_Arm/4.ctrl_servo.ipynb\n</code></pre> </li> <li> <p>Click \u201cRun All\u201d in the toolbar to execute the entire notebook.</p> </li> <li> <p>The notebook will continuously print the current angles of all six servos in real time.</p> </li> <li> <p>It will also move the servo with ID = 6 to 150\u00b0, then display its updated angle.</p> </li> <li> <p>To stop the program, click the \u201cStop\u201d button in the JupyterLab toolbar.</p> </li> </ol>"},{"location":"Dofbot-SE/04servo_position/#4-notes","title":"4. Notes","text":"<ul> <li> <p>Servo angle readings update dynamically, allowing you to monitor the arm\u2019s posture in real time.</p> </li> <li> <p>Frequent reads (short delays) may increase CPU load; adjust the interval (<code>time.sleep()</code>) as needed.</p> </li> <li> <p>The returned angle values are measured from the servo\u2019s internal position sensor.</p> </li> <li> <p>Always ensure that the servo ID is valid (1\u20136) before calling this function.</p> </li> <li> <p>Use <code>del Arm</code> at the end of the script to safely close the device connection.</p> </li> </ul>"},{"location":"Dofbot-SE/05six_servos/","title":"Controlling All Six Servos Simultaneously","text":""},{"location":"Dofbot-SE/05six_servos/#1-api-overview","title":"1. API Overview","text":"<p>To control all six bus servos of the robotic arm at once, use the following API:</p>"},{"location":"Dofbot-SE/05six_servos/#arm_serial_servo_write6s1-s2-s3-s4-s5-s6-time","title":"<code>Arm_serial_servo_write6(S1, S2, S3, S4, S5, S6, time)</code>","text":"<p>Function: Controls all six servos simultaneously, moving each one to its specified target angle.</p> <p>Parameters:</p> Parameter Description Range S1 Servo 1 target angle 0\u2013180\u00b0 S2 Servo 2 target angle 0\u2013180\u00b0 S3 Servo 3 target angle 0\u2013180\u00b0 S4 Servo 4 target angle 0\u2013180\u00b0 S5 Servo 5 target angle 0\u2013270\u00b0 S6 Servo 6 target angle 0\u2013180\u00b0 time Duration (ms) \u2014 time for all servos to complete movement \u22650 <p>Notes:</p> <ul> <li> <p>The smaller the time value, the faster the movement.</p> </li> <li> <p>Setting time = 0 makes the servos move at maximum speed.</p> </li> <li> <p>The function does not return any value.</p> </li> </ul>"},{"location":"Dofbot-SE/05six_servos/#2-example-code","title":"2. Example Code","text":"<p>File path: <code>/home/yahboom/Dofbot/3.ctrl_Arm/5.ctrl_all.ipynb</code></p> <pre><code>#!/usr/bin/env python3\n# coding=utf-8\n\nimport time\nfrom Arm_Lib import Arm_Device\n\n# Create the Arm device object\nArm = Arm_Device()\ntime.sleep(0.1)\n\n# Function to control all six servos simultaneously\ndef ctrl_all_servos(angle, s_time=500):\n    \"\"\"\n    Move all six servos to specific angles.\n    For demonstration, servo 2 moves in the opposite direction of servo 1.\n    \"\"\"\n    Arm.Arm_serial_servo_write6(\n        angle,             # Servo 1\n        180 - angle,       # Servo 2 (mirror motion)\n        angle,             # Servo 3\n        angle,             # Servo 4\n        angle,             # Servo 5\n        angle,             # Servo 6\n        s_time             # Duration (ms)\n    )\n    time.sleep(s_time / 1000.0)\n\ndef main():\n    direction = 1\n    angle = 90\n\n    # Reset all servos to center position\n    Arm.Arm_serial_servo_write6(90, 90, 90, 90, 90, 90, 500)\n    time.sleep(1)\n\n    # Loop to continuously move all servos back and forth\n    while True:\n        if direction == 1:\n            angle += 1\n            if angle &gt;= 180:\n                direction = 0\n        else:\n            angle -= 1\n            if angle &lt;= 0:\n                direction = 1\n\n        ctrl_all_servos(angle, 10)\n        time.sleep(0.01)  # Slight delay to control movement smoothness\n\ntry:\n    main()\nexcept KeyboardInterrupt:\n    print(\"Program closed by user.\")\n    pass\n\n# Release the Arm device resource\ndel Arm\n</code></pre>"},{"location":"Dofbot-SE/05six_servos/#3-how-to-run-in-jupyterlab","title":"3. How to Run in JupyterLab","text":"<ol> <li> <p>Open JupyterLab and navigate to:</p> <pre><code>/home/yahboom/Dofbot/3.ctrl_Arm/5.ctrl_all.ipynb\n</code></pre> </li> <li> <p>Click \u201cRun All\u201d in the toolbar to execute the entire notebook.</p> </li> <li> <p>The robotic arm\u2019s six servos will move simultaneously, changing the arm\u2019s posture continuously.</p> <ul> <li>The motion alternates between 0\u00b0 and 180\u00b0, creating a smooth oscillating effect.</li> </ul> </li> <li> <p>To stop the motion, click the \u201cStop\u201d button on the JupyterLab toolbar.</p> </li> </ol>"},{"location":"Dofbot-SE/05six_servos/#4-notes","title":"4. Notes","text":"<ul> <li> <p>Use this function to perform synchronized, multi-joint movements for coordinated robotic gestures.</p> </li> <li> <p>Adjust the <code>time</code> parameter to balance between speed and stability.</p> </li> <li> <p>Avoid setting extremely fast speeds for large angle movements \u2014 this can cause vibration or instability.</p> </li> <li> <p>Always reset the arm to a neutral position (90\u00b0, 90\u00b0, 90\u00b0, 90\u00b0, 90\u00b0, 90\u00b0) before complex motions.</p> </li> <li> <p>After the script finishes, use <code>del Arm</code> to safely release hardware control.</p> </li> </ul>"},{"location":"Dofbot-SE/05six_servos/#5-example-applications","title":"5. Example Applications","text":"<ul> <li> <p>Coordinated arm movement demonstrations</p> </li> <li> <p>Gesture playback or motion recording systems</p> </li> <li> <p>Smooth transitions between saved postures</p> </li> <li> <p>Basic testing of servo synchronization and performance</p> </li> </ul>"},{"location":"Dofbot-SE/06/","title":"06","text":"<pre><code>#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n------------------------------------------------------------\nDofbot Arm Control Experiment 6.6 \u2014 Up, Down, Left, and Right Swing\n------------------------------------------------------------\nAuthor: Yahboom\nFile Path: /home/yahboom/Dofbot/3.ctrl_Arm/6.left_right.ipynb\nDescription:\n    This experiment demonstrates how to make the robotic arm\n    swing up, down, left, and right, and then return to its\n    upright (centered) position.\n\nExperiment Idea:\n    - Control servo #3 and servo #4 simultaneously to achieve\n      up-and-down movement of the arm.\n    - Control servo #1 to swing the arm left and right.\n    - Finally, reset all servos to the neutral 90\u00b0 position.\n------------------------------------------------------------\n\"\"\"\n\nimport time\nfrom Arm_Lib import Arm_Device\n\n\ndef main():\n    \"\"\"\n    Main control function for the Dofbot arm.\n\n    The function performs a continuous loop:\n        1. Reset all servos to the middle position (90\u00b0).\n        2. Move servos #3 and #4 for up/down motion.\n        3. Move servo #1 for left/right swinging.\n        4. Return the arm to the upright position.\n    \"\"\"\n\n    # Create an instance of the Arm_Device class\n    arm = Arm_Device()\n    time.sleep(0.1)\n\n    # Step 1: Reset all servos to their neutral positions\n    arm.Arm_serial_servo_write6(90, 90, 90, 90, 90, 90, 500)\n    time.sleep(1)\n\n    try:\n        while True:\n            # Step 2: Move servos #3 and #4 to swing up/down\n            # Servo #3 -&gt; 0\u00b0, Servo #4 -&gt; 180\u00b0\n            arm.Arm_serial_servo_write(3, 0, 1000)\n            time.sleep(0.001)\n            arm.Arm_serial_servo_write(4, 180, 1000)\n            time.sleep(1)\n\n            # Step 3: Move servo #1 to swing left/right\n            # Servo #1 -&gt; right (180\u00b0)\n            arm.Arm_serial_servo_write(1, 180, 500)\n            time.sleep(0.5)\n\n            # Servo #1 -&gt; left (0\u00b0)\n            arm.Arm_serial_servo_write(1, 0, 1000)\n            time.sleep(1)\n\n            # Step 4: Reset all servos to neutral position again\n            arm.Arm_serial_servo_write6(90, 90, 90, 90, 90, 90, 1000)\n            time.sleep(1.5)\n\n    except KeyboardInterrupt:\n        print(\"\\nProgram closed by user.\")\n    finally:\n        # Always release the arm object when exiting\n        del arm\n        print(\"Arm object released.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"Dofbot-SE/06/#key-improvements","title":"\u2705 Key Improvements","text":"Aspect Description Documentation Added full docstring at the top and in the <code>main()</code> function to explain purpose and logic. Naming &amp; Readability Changed <code>Arm</code> \u2192 <code>arm</code> to follow Python naming conventions (lowercase variables). Structure Wrapped logic in <code>if __name__ == \"__main__\":</code> for better modularity. Error Handling Added <code>try\u2013except\u2013finally</code> for safe cleanup. Timing Consistency Used standard decimal format for <code>time.sleep()</code> values. Comments Each movement step is clearly documented (purpose + effect)."},{"location":"Dofbot-SE/07/","title":"Arm Motion Memory","text":""},{"location":"Dofbot-SE/07/#1-api-overview","title":"1. API Overview","text":"<p>This section introduces the APIs related to motion learning and playback of the robotic arm.</p>"},{"location":"Dofbot-SE/07/#11-enter-or-exit-learning-mode","title":"1.1 Enter or Exit Learning Mode","text":"<p>Function:</p> <pre><code>Arm_Button_Mode(enable)\n</code></pre> <p>Description: Enables or disables the robotic arm\u2019s learning mode.</p> <p>Parameters:</p> <ul> <li> <p><code>enable</code>:</p> <ul> <li> <p><code>0</code> \u2014 Exit learning mode</p> </li> <li> <p><code>1</code> \u2014 Enter learning mode</p> </li> </ul> </li> </ul> <p>Behavior:</p> <ul> <li> <p>When entering learning mode:</p> <ul> <li> <p>The RGB LED on the control board enters a breathing light state.</p> </li> <li> <p>The arm automatically disables torque, allowing manual adjustment of joint angles.</p> </li> <li> <p>Each time the K1 button on the board is pressed, the breathing light color changes, and the current arm pose is recorded.</p> </li> <li> <p>Up to 20 motion groups can be stored. When this limit is reached, the RGB light turns red, and no further poses can be recorded.</p> </li> </ul> </li> <li> <p>When exiting learning mode:</p> <ul> <li> <p>Torque is re-enabled.</p> </li> <li> <p>The RGB LED is turned off.</p> </li> </ul> </li> </ul> <p>Return Value: None</p>"},{"location":"Dofbot-SE/07/#12-read-the-number-of-recorded-actions","title":"1.2 Read the Number of Recorded Actions","text":"<p>Function:</p> <pre><code>Arm_Read_Action_Num()\n</code></pre> <p>Description: Reads the number of currently recorded motion groups.</p> <p>Return Value: The total number of saved motion groups.</p>"},{"location":"Dofbot-SE/07/#13-run-recorded-actions","title":"1.3 Run Recorded Actions","text":"<p>Function:</p> <pre><code>Arm_Action_Mode(mode)\n</code></pre> <p>Description: Executes recorded motion sequences.</p> <p>Parameters:</p> <ul> <li> <p><code>mode</code>:</p> <ul> <li> <p><code>0</code> \u2014 Stop running actions</p> </li> <li> <p><code>1</code> \u2014 Run the recorded motions once</p> </li> <li> <p><code>2</code> \u2014 Loop through all recorded motions continuously</p> </li> </ul> </li> </ul> <p>Return Value: None</p>"},{"location":"Dofbot-SE/07/#14-clear-recorded-actions","title":"1.4 Clear Recorded Actions","text":"<p>Function:</p> <pre><code>Arm_Clear_Action()\n</code></pre> <p>Description: Clears all recorded motion groups. \u26a0\ufe0f This operation cannot be undone.</p> <p>Return Value: None</p>"},{"location":"Dofbot-SE/07/#15-record-a-motion-in-learning-mode","title":"1.5 Record a Motion in Learning Mode","text":"<p>Function:</p> <pre><code>Arm_Action_Study()\n</code></pre> <p>Description: In learning mode, records the current arm pose as a new motion group. When the recording succeeds, the RGB breathing light changes color.</p> <p>Return Value: None</p>"},{"location":"Dofbot-SE/07/#2-code-example","title":"2. Code Example","text":"<p>File Path:</p> <pre><code>/home/yahboom/Dofbot/3.ctrl_Arm/8.study_mode.ipynb\n</code></pre> <p>Important: Run each cell step-by-step in JupyterLab. Do not execute all cells at once.</p> <pre><code>#!/usr/bin/env python3\n# coding=utf-8\n\nimport time\nfrom Arm_Lib import Arm_Device\n\n# Create the robotic arm object\nArm = Arm_Device()\ntime.sleep(0.1)\n\n# --- Step 1: Enable learning mode ---\n# The RGB LED enters breathing mode, and all servos disable torque,\n# allowing free manual movement of the arm.\nArm.Arm_Button_Mode(1)\n\n# --- Step 2: Record current pose ---\n# Each call to this function saves the current arm pose.\n# The RGB color changes to confirm success.\n# When the red light appears, the maximum (20) has been reached.\n# You can also press the K1 button on the board instead of calling this function.\nArm.Arm_Action_Study()\n\n# --- Step 3: Exit learning mode ---\n# Disables breathing light and re-enables torque.\nArm.Arm_Button_Mode(0)\n\n# --- Step 4: Read the number of recorded actions ---\nnum = Arm.Arm_Read_Action_Num()\nprint(num)\n\n# --- Step 5: Run the recorded motions ---\nArm.Arm_Action_Mode(1)   # Run once\nArm.Arm_Action_Mode(2)   # Run in a loop\nArm.Arm_Action_Mode(0)   # Stop running\n\n# --- Step 6: Clear all recorded actions ---\n# \u26a0\ufe0f This cannot be undone. The RGB light turns green when clearing is complete.\nArm.Arm_Clear_Action()\n\n# --- Step 7: Release the Arm object ---\ndel Arm\n</code></pre>"},{"location":"Dofbot-SE/07/#3-operation-notes","title":"3. Operation Notes","text":"<p>In JupyterLab, open this notebook and run each cell sequentially:</p> <ol> <li> <p>Enable learning mode.</p> </li> <li> <p>Manually move the robotic arm to desired poses.</p> </li> <li> <p>Call <code>Arm.Arm_Action_Study()</code> (or press K1) to record each pose.</p> </li> <li> <p>After recording several poses, disable learning mode.</p> </li> <li> <p>Use the above commands to run, loop, or clear the recorded motions.</p> </li> </ol> <p>Once completed, you can observe the robotic arm performing the learned motion sequences automatically.</p>"},{"location":"FaceSwap/pt1/","title":"Part 1","text":""},{"location":"FaceSwap/pt1/#original-code","title":"original code","text":"<pre><code>import cv2  \nimport numpy as np  \nimport dlib  \n\nimg = cv2.imread(\"face.jpg\")  \nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  \nmask = np.zeros_like(img_gray)  \n\ndetector = dlib.get_frontal_face_detector()  \npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  \n\nfaces = detector(img_gray)  \nfor face in faces:  \n    landmarks = predictor(img_gray, face)  \n    landmarks_points = []  \n\n    for i in range(68):  \n        x, y = landmarks.part(i).x, landmarks.part(i).y  \n        landmarks_points.append((x, y))  \n\n        # cv2.circle(img, (x, y), 3, (0, 0, 255), -1)  \n\n    points = np.array(landmarks_points)  \n    convex_hull = cv2.convexHull(points)  \n    # cv2.polylines(img, [convex_hull], True, (255, 0, 0), 2)  \n\n    cv2.fillConvexPoly(mask, convex_hull, 255)  \n\n    face_image_1 = cv2.bitwise_and(img, img, mask=mask)  \n\ncv2.imshow(\"Image\", img)  \ncv2.imshow(\"Face Image 1\", face_image_1)  \ncv2.waitKey(0)  \ncv2.destroyAllWindows()  \ncv2.imshow(\"Mask\", mask)  \ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n</code></pre> <p>That code snippet uses Dlib and OpenCV for Face Landmark Detection and then extracts the face region using a Convex Hull and a mask.</p> <p>Here is the rewritten code with comments, followed by a post explaining the technology.</p>"},{"location":"FaceSwap/pt1/#rewritten-and-commented-code","title":"\ud83d\udcbb Rewritten and Commented Code","text":"<p>This script detects a face in an image, finds 68 key facial landmarks, uses these landmarks to create a convex hull around the face, and then uses this hull to generate a mask for isolating the face region.</p> <pre><code>import cv2        # OpenCV for image processing functions\nimport numpy as np    # NumPy for numerical operations, especially array manipulation\nimport dlib       # Dlib for face detection and landmark prediction\n\n# --- 1. Load Image and Initialize Variables ---\n\n# Load the input image. Ensure \"face.jpg\" is in the same directory.\n# Replace \"face.jpg\" with your image file name.\nIMAGE_PATH = \"face.jpg\"\nimg = cv2.imread(IMAGE_PATH)\n\n# Check if the image was loaded successfully\nif img is None:\n    print(f\"Error: Could not open or find the image at {IMAGE_PATH}\")\n    exit()\n\n# Convert the image to grayscale, as Dlib detectors usually work on grayscale images\nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Create a black mask image of the same size as the grayscale image, initialized to zeros.\n# This mask will be used to isolate the face area.\nmask = np.zeros_like(img_gray)\n\n# --- 2. Initialize Dlib Models ---\n\n# Initialize Dlib's default face detector (a Histogram of Oriented Gradients (HOG) based detector)\ndetector = dlib.get_frontal_face_detector()\n\n# Initialize Dlib's 68-point shape predictor.\n# The \"shape_predictor_68_face_landmarks.dat\" file must be present.\nPREDICTOR_PATH = \"shape_predictor_68_face_landmarks.dat\"\ntry:\n    predictor = dlib.shape_predictor(PREDICTOR_PATH)\nexcept RuntimeError as e:\n    print(f\"Error loading predictor: {e}\")\n    print(f\"Ensure '{PREDICTOR_PATH}' is available.\")\n    exit()\n\n\n# --- 3. Detect Faces and Process Landmarks ---\n\n# Detect faces in the grayscale image\nfaces = detector(img_gray)\n\n# Iterate through all detected faces\nfor face in faces:\n    # Use the predictor to find the 68 facial landmarks for the current face\n    landmarks = predictor(img_gray, face)\n    landmarks_points = [] # List to store the (x, y) coordinates of the landmarks\n\n    # Extract the (x, y) coordinates of all 68 landmarks\n    for i in range(68):\n        x = landmarks.part(i).x\n        y = landmarks.part(i).y\n        landmarks_points.append((x, y))\n\n        # Optional: Uncomment to draw the landmarks on the original image\n        # cv2.circle(img, (x, y), 3, (0, 0, 255), -1)\n\n    # Convert the list of points into a NumPy array suitable for OpenCV functions\n    points = np.array(landmarks_points, np.int32)\n\n    # Calculate the Convex Hull (the smallest convex polygon that encloses all landmark points)\n    convex_hull = cv2.convexHull(points)\n\n    # Optional: Uncomment to draw the convex hull boundary on the original image\n    # cv2.polylines(img, [convex_hull], True, (255, 0, 0), 2)\n\n    # Fill the mask with white (255) in the area defined by the convex hull.\n    # This creates a solid white face shape on the black mask.\n    cv2.fillConvexPoly(mask, convex_hull, 255)\n\n    # Use the mask to extract only the face region from the original image (img).\n    # The mask ensures that only pixels where 'mask' is 255 are kept (white pixels in mask).\n    face_image_1 = cv2.bitwise_and(img, img, mask=mask)\n\n    # Since 'face_image_1' is created inside the loop, if there are multiple faces,\n    # it will only contain the face processed last. If the goal is to process all faces,\n    # this part would need modification (e.g., storing all results or iterating differently).\n\n\n# --- 4. Display Results ---\n\n# Display the original image (with optional drawn landmarks/hull)\ncv2.imshow(\"1. Original Image\", img)\n\n# Display the generated mask\ncv2.imshow(\"2. Face Convex Hull Mask\", mask)\n\n# Display the extracted face image\n# This variable might not be defined if no faces were found.\ntry:\n    cv2.imshow(\"3. Extracted Face Image (using Mask)\", face_image_1)\nexcept NameError:\n    print(\"No faces detected or 'face_image_1' was not created.\")\n\n# Wait indefinitely for a key press\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"FaceSwap/pt1/#tech-post-behind-the-face-isolation-code","title":"\ud83d\udcf0 Tech Post: Behind the Face Isolation Code","text":""},{"location":"FaceSwap/pt1/#unmasking-the-face-a-look-at-face-landmark-detection-and-image-segmentation","title":"Unmasking the Face: A Look at Face Landmark Detection and Image Segmentation \ud83e\udd16","text":"<p>Have you ever wondered how Snapchat filters work, or how deepfake software can precisely track facial movements? It all starts with accurately locating a face and its features. The code above demonstrates a fundamental technique in Computer Vision for not only finding a face but also isolating it from the background.</p>"},{"location":"FaceSwap/pt1/#whats-happening-under-the-hood","title":"What's Happening Under the Hood?","text":"<p>The core of this process relies on two powerful libraries: Dlib and OpenCV.</p>"},{"location":"FaceSwap/pt1/#1-face-detection-with-dlib","title":"1. Face Detection with Dlib","text":"<p>The first step is to locate any human faces in the image. We use Dlib's \\(\\text{get\\_frontal\\_face\\_detector()}\\), which is typically a pre-trained Histogram of Oriented Gradients (HOG) and Support Vector Machine (SVM) model.</p> <ul> <li> <p>The HOG algorithm detects the shape and appearance of objects (like faces) by examining the distribution of intensity gradients (how sharp the color changes are).</p> </li> <li> <p>The SVM is then used to classify whether the detected region is, in fact, a face.</p> </li> </ul> <p>This detector returns a bounding box (rectangle) for every face it finds.</p>"},{"location":"FaceSwap/pt1/#2-pinpointing-features-with-shape-prediction","title":"2. Pinpointing Features with Shape Prediction","text":"<p>Once a face is found, the \\(\\text{shape\\_predictor()}\\) model takes over. This is a crucial step known as Face Landmark Detection (or Face Alignment).</p> <ul> <li> <p>Using a pre-trained model file (\\(\\text{shape\\_predictor\\_68\\_face\\_landmarks.dat}\\)), the predictor analyzes the area within the detected bounding box.</p> </li> <li> <p>It precisely locates 68 key points on the face, including the corners of the eyes, the tip of the nose, and the outline of the jaw and mouth. These points provide a detailed map of the face's geometry.</p> </li> </ul>"},{"location":"FaceSwap/pt1/#3-creating-the-face-outline-with-convex-hull","title":"3. Creating the Face Outline with Convex Hull","text":"<p>The 68 landmarks define the shape of the face. To get a clean, solid outline of the entire facial area, the algorithm computes the Convex Hull of these 68 points using OpenCV's \\(\\text{cv2.convexHull()}\\) function.</p> <ul> <li>A Convex Hull is the smallest convex polygon that contains all of the landmark points. Imagine stretching a rubber band around all the points; the shape the rubber band forms is the convex hull. This efficiently gives us a smooth, closed boundary for the face.</li> </ul>"},{"location":"FaceSwap/pt1/#4-isolation-via-bitwise-operations-and-a-mask","title":"4. Isolation via Bitwise Operations and a Mask","text":"<p>Finally, we use the convex hull to segment the face.</p> <ol> <li> <p>Generate a Mask: A separate black image (the mask) is created. The \\(\\text{cv2.fillConvexPoly()}\\) function then draws and fills the convex hull shape onto this mask with white color (pixel value 255). We now have a mask where the face area is white and everything else is black.</p> </li> <li> <p>Apply the Mask: The \\(\\text{cv2.bitwise\\_and()}\\) operation is the trick. It performs a logical AND between the original color image and itself, but only where the corresponding pixel in the mask is white (non-zero). This results in an output image where:</p> <ul> <li> <p>Pixels inside the face hull (mask is white) keep their original color.</p> </li> <li> <p>Pixels outside the face hull (mask is black) become black (zero).</p> </li> </ul> </li> </ol> <p>The result is a clean image of the isolated face, ready for further processing, analysis, or application of visual effects.</p>"},{"location":"FaceSwap/pt1/#key-takeaway","title":"Key Takeaway","text":"<p>This simple script combines Dlib's machine learning power for feature localization with OpenCV's image manipulation tools to perform accurate face segmentation. It is a foundational technique in countless real-world applications, from biometrics to augmented reality.</p>"},{"location":"FaceSwap/pt2/","title":"Part 2","text":""},{"location":"FaceSwap/pt2/#original-code","title":"original code","text":"<pre><code>import cv2  \nimport numpy as np  \nimport dlib  \n\nimg = cv2.imread(\"face.jpg\")  \nimg_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  \nmask = np.zeros_like(img_gray)  \n\ndetector = dlib.get_frontal_face_detector()  \npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  \n\nfaces = detector(img_gray)  \nfor face in faces:  \n    landmarks = predictor(img_gray, face)  \n    landmarks_points = []  \n\n    for i in range(68):  \n        x, y = landmarks.part(i).x, landmarks.part(i).y  \n        landmarks_points.append((x, y))  \n\n        # cv2.circle(img, (x, y), 3, (0, 0, 255), -1)  \n\n    points = np.array(landmarks_points)  \n    convex_hull = cv2.convexHull(points)  \n    # cv2.polylines(img, [convex_hull], True, (255, 0, 0), 2)  \n\n    cv2.fillConvexPoly(mask, convex_hull, 255)  \n\n    face_image_1 = cv2.bitwise_and(img, img, mask=mask)  \n\n    # Delaunay triangulation  \n    rect = cv2.boundingRect(convex_hull)  \n    # (x, y, w, h) = rect  \n    # cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)  \n    subdiv = cv2.Subdiv2D(rect)  \n    subdiv.insert(landmarks_points)  \n    triangles = subdiv.getTriangleList()  \n    triangles = np.array(triangles, dtype=np.int32)  \n\n    for t in triangles:  \n        pt1 = (t[0], t[1])  \n        pt2 = (t[2], t[3])  \n        pt3 = (t[4], t[5])  \n\n        cv2.line(img, pt1, pt2, (0, 255, 0), 1)  \n        cv2.line(img, pt2, pt3, (0, 255, 0), 1)  \n        cv2.line(img, pt3, pt1, (0, 255, 0), 1)  \n\ncv2.imshow(\"Image\", img)  \n# cv2.imshow(\"Face Image 1\", face_image_1)  \n# cv2.imshow(\"Mask\", mask)  \ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n</code></pre>"},{"location":"OpenCV/01-brief-intro-to-opencv/","title":"Brief Intro to OpenCV","text":"<p>OpenCV (Open Source Computer Vision Library) is an incredibly powerful, open-source library that's become the gold standard for computer vision tasks across academic, research, and commercial applications.</p>"},{"location":"OpenCV/01-brief-intro-to-opencv/#what-is-computer-vision","title":"What is Computer Vision?","text":"<p>Computer vision is a field of computer science that deals with how computers can gain high-level understanding from digital images or videos. Essentially, it teaches computers how to \"see\" and interpret the visual world.</p>"},{"location":"OpenCV/01-brief-intro-to-opencv/#key-capabilities-of-opencv","title":"Key Capabilities of OpenCV","text":"<p>OpenCV provides a massive collection of over 2,500 optimized algorithms. Some of the most common applications include:</p> <ul> <li> <p>Image Processing: Performing basic operations like resizing, cropping, color space conversion (e.g., RGB to grayscale), and applying filters.</p> </li> <li> <p>Object Detection: Identifying and locating specific objects within an image or video frame (e.g., detecting faces, cars, or specific items).</p> </li> <li> <p>Object Recognition: Identifying what the object is (e.g., recognizing a specific person's face).</p> </li> <li> <p>Video Analysis: Tracking moving objects, analyzing motion, and performing background subtraction.</p> </li> <li> <p>3D Reconstruction: Estimating 3D structure from 2D images.</p> </li> <li> <p>Machine Learning Integration: It has built-in support for some common machine learning algorithms often used in computer vision.</p> </li> </ul>"},{"location":"OpenCV/01-brief-intro-to-opencv/#why-is-opencv-popular","title":"Why is OpenCV Popular?","text":"<ol> <li> <p>Performance: It's primarily written in C++, which means its algorithms are very fast and optimized for real-time applications.</p> </li> <li> <p>Multi-language Support: While the core is C++, it offers comprehensive interfaces for Python, Java, and MATLAB, making it accessible to a wide range of developers.</p> </li> <li> <p>Cross-platform: It runs on Windows, Linux, macOS, Android, and iOS.</p> </li> <li> <p>Open Source: It's free to use under a permissive license (Apache 2 License), encouraging widespread adoption and contribution.</p> </li> </ol>"},{"location":"OpenCV/01-brief-intro-to-opencv/#getting-started","title":"Getting Started","text":"<p>The easiest way to begin with OpenCV is typically through the Python interface (<code>opencv-python</code>). You can install it with a simple <code>pip</code> command and start experimenting with reading images, converting colors, and applying basic filters!</p> <p>OpenCV is your essential toolkit for making computers see!</p>"},{"location":"OpenCV/02-common-usage/","title":"\ud83d\udc0d Common OpenCV Usage Examples in Python","text":"<p>OpenCV is typically used in Python via the <code>cv2</code> module. Here are some of the most common, fundamental operations you'll encounter.</p>"},{"location":"OpenCV/02-common-usage/#1-reading-displaying-and-writing-images","title":"1. Reading, Displaying, and Writing Images","text":"<p>This is the most basic workflow for any image processing task.</p> <ul> <li>im: image</li> </ul> Function Purpose Example Usage <code>cv2.imread()</code> Loads an image from a file. <code>img = cv2.imread('image.jpg')</code> <code>cv2.imshow()</code> Displays the image in a window. <code>cv2.imshow('My Image', img)</code> <code>cv2.waitKey()</code> Waits for a key press. Essential for the display window to stay open. <code>cv2.waitKey(0)</code> <code>cv2.imwrite()</code> Saves the processed image to a file. <code>cv2.imwrite('new_image.png', img)</code> <code>cv2.destroyAllWindows()</code> Closes all open windows. <code>cv2.destroyAllWindows()</code>"},{"location":"OpenCV/02-common-usage/#2-color-space-conversion","title":"2. Color Space Conversion","text":"<p>A frequent task is converting a standard color image (BGR in OpenCV, not RGB!) to Grayscale for simpler processing, or converting to the HSV (Hue, Saturation, Value) color space for easier color-based segmentation.</p> <ul> <li>Grayscale Conversion: Reduces the image from three channels (Blue, Green, Red) to one intensity channel.</li> </ul> <pre><code>gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n</code></pre> <ul> <li>HSV Conversion: Useful for isolating colors because the Hue channel is separated from brightness/intensity (Value).</li> </ul> <pre><code>hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n</code></pre>"},{"location":"OpenCV/02-common-usage/#3-resizing-and-scaling-images","title":"3. Resizing and Scaling Images","text":"<p>To standardize input sizes for algorithms or to speed up processing, resizing is crucial.</p> <ul> <li>By Fixed Size: Resize to a specific width and height.</li> </ul> <pre><code>resized = cv2.resize(img, (200, 300)) # (width, height)\n</code></pre> <ul> <li>By Scale Factor: Scale the image by a factor (e.g., half its size).</li> </ul> <pre><code>scaled = cv2.resize(img, None, fx=0.5, fy=0.5)\n</code></pre>"},{"location":"OpenCV/02-common-usage/#4-simple-filteringblurring-smoothing","title":"4. Simple Filtering/Blurring (Smoothing)","text":"<p>Filters are used to remove noise or smooth an image. The Gaussian Blur is one of the most common smoothing filters.</p> <ul> <li>The second argument <code>(5, 5)</code> is the kernel size (must be odd and positive). A larger kernel results in a greater blur.</li> </ul> <pre><code>blurred_img = cv2.GaussianBlur(img, (5, 5), 0)\n</code></pre>"},{"location":"OpenCV/02-common-usage/#5-drawing-shapes-and-text","title":"5. Drawing Shapes and Text","text":"<p>OpenCV lets you draw geometric shapes or text directly onto an image, often used to visualize detection results.</p> Function Purpose <code>cv2.rectangle()</code> Draws a rectangle (often used for Bounding Boxes). <code>cv2.circle()</code> Draws a circle (useful for keypoints). <code>cv2.line()</code> Draws a line segment. <code>cv2.putText()</code> Draws text on the image. <p>Example (Drawing a Red Rectangle):</p> <pre><code># img, start_point, end_point, color (BGR), thickness\ncv2.rectangle(img, (100, 100), (300, 300), (0, 0, 255), 2)\n</code></pre>"},{"location":"OpenCV/02-common-usage/#6-edge-detection","title":"6. Edge Detection","text":"<p>Canny Edge Detection is a popular multi-stage algorithm used to find a wide range of edges in images, making them suitable for subsequent processing.</p> <pre><code># The last two arguments are the minimum and maximum threshold values\nedges = cv2.Canny(gray_img, 100, 200)\n</code></pre>"},{"location":"OpenCV/02-common-usage/#7-video-processing-and-webcam-access","title":"7. Video Processing and Webcam Access","text":"<p>OpenCV treats a video as a sequence of individual images (frames).</p> <ul> <li>Reading Video/Webcam: Use <code>cv2.VideoCapture()</code>. Pass the video file path or <code>0</code> for the default webcam.</li> </ul> <pre><code>cap = cv2.VideoCapture(0) # 0 for webcam\n\nwhile True:\n    ret, frame = cap.read() # Read a frame\n    if not ret:\n        break\n\n    cv2.imshow('Video Feed', frame)\n\n    # Exit loop on 'q' key press\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"OpenCV/attachments/","title":"Attachments","text":"<ul> <li>ZoranDuric-2015-OpenCV_in_Python-GMU.pdf   https://cs.gmu.edu/~kosecka/cs482/code-examples/opencv-python/OpenCV_Python.pdf</li> </ul>"},{"location":"OpenCV/opencv/","title":"Introduction to OpenCV","text":"<p>OpenCV (Open Source Computer Vision Library) is one of the most powerful and widely used libraries for computer vision, image processing, and machine learning.  </p> <p>It provides tools to capture, manipulate, analyze, and understand images and videos \u2014 enabling computers to \"see\" and interpret the visual world.</p> <p>OpenCV is written in C++, but it offers robust bindings for Python, Java, and other languages. For beginners, Python + OpenCV is the most accessible and widely used combination.</p>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#1-what-is-opencv","title":"1. What Is OpenCV?","text":"<p>OpenCV was originally developed by Intel in 1999 to promote real-time computer vision research. Today, it\u2019s maintained by the OpenCV Foundation and supported by a large open-source community.</p> <p>Its goal is to provide a unified framework for visual computing \u2014 from simple image transformations to advanced object detection and deep learning.</p>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#example-capabilities","title":"Example Capabilities","text":"<ul> <li>Image reading, writing, and displaying  </li> <li>Color space conversions  </li> <li>Edge, contour, and feature detection  </li> <li>Object recognition and tracking  </li> <li>Face and motion detection  </li> <li>Integration with deep learning frameworks (TensorFlow, PyTorch, ONNX)</li> </ul>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#2-installation","title":"2. Installation","text":"","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#python-installation","title":"Python Installation","text":"<p>You can install OpenCV easily with <code>pip</code>:</p> <pre><code>pip install opencv-python\n</code></pre> <p>To include advanced (non-free) features like SIFT or SURF, install:</p> <pre><code>pip install opencv-contrib-python\n</code></pre> <p>Verify installation:</p> <pre><code>import cv2\nprint(cv2.__version__)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#3-core-concepts","title":"3. Core Concepts","text":"","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#31-image-representation","title":"3.1 Image Representation","text":"<p>In OpenCV, an image is represented as a NumPy array, where each pixel corresponds to a set of color values.</p> <ul> <li> <p>Grayscale image: 2D array (height \u00d7 width)</p> </li> <li> <p>Color image (BGR): 3D array (height \u00d7 width \u00d7 3)</p> </li> </ul> <p>Example:</p> <pre><code>import cv2\n\nimg = cv2.imread('example.jpg')  # Load image\nprint(img.shape)  # Output: (height, width, 3)\n</code></pre> <p>\u26a0\ufe0f OpenCV uses BGR (Blue-Green-Red) order instead of RGB.</p>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#32-reading-displaying-and-saving-images","title":"3.2 Reading, Displaying, and Saving Images","text":"<pre><code>import cv2\n\n# Read an image\nimg = cv2.imread('cat.jpg')\n\n# Display it in a window\ncv2.imshow('Cat', img)\ncv2.waitKey(0)  # Wait for key press\ncv2.destroyAllWindows()\n\n# Save the image\ncv2.imwrite('cat_copy.jpg', img)\n</code></pre> <ul> <li><code>cv.imread()</code></li> <li><code>cv.imshow()</code></li> <li><code>cv.imwrite()</code></li> <li><code>cv.waitKey()</code></li> <li><code>cv.destroyAllWindows()</code></li> </ul>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#33-video-capture","title":"3.3 Video Capture","text":"<p>You can capture video from a file or a webcam.</p> <pre><code>import cv2\n\ncap = cv2.VideoCapture(0)  # 0 = default webcam\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    cv2.imshow('Camera', frame)\n\n    if cv2.waitKey(1) == ord('q'):  # Press 'q' to quit\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#4-basic-image-operations","title":"4. Basic Image Operations","text":"","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#41-resize-and-crop","title":"4.1 Resize and Crop","text":"<pre><code>resized = cv2.resize(img, (300, 200))\ncropped = img[50:200, 100:300]\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#42-color-conversion","title":"4.2 Color Conversion","text":"<pre><code>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nhsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#43-drawing-shapes-and-text","title":"4.3 Drawing Shapes and Text","text":"<pre><code>cv2.line(img, (0, 0), (150, 150), (255, 0, 0), 3)\ncv2.rectangle(img, (50, 50), (200, 200), (0, 255, 0), 2)\ncv2.circle(img, (150, 150), 50, (0, 0, 255), -1)\ncv2.putText(img, 'OpenCV', (50, 250), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#5-image-processing-techniques","title":"5. Image Processing Techniques","text":"","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#51-blurring","title":"5.1 Blurring","text":"<pre><code>blur = cv2.GaussianBlur(img, (5, 5), 0)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#52-edge-detection","title":"5.2 Edge Detection","text":"<pre><code>edges = cv2.Canny(img, 100, 200)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#53-thresholding","title":"5.3 Thresholding","text":"<pre><code>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n_, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#54-contour-detection","title":"5.4 Contour Detection","text":"<pre><code>contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncv2.drawContours(img, contours, -1, (0, 255, 0), 2)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#6-geometric-transformations","title":"6. Geometric Transformations","text":"","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#61-translation","title":"6.1 Translation","text":"<pre><code>M = np.float32([[1, 0, 100], [0, 1, 50]])  # Move 100px right, 50px down\nshifted = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#62-rotation","title":"6.2 Rotation","text":"<pre><code>(h, w) = img.shape[:2]\ncenter = (w // 2, h // 2)\nM = cv2.getRotationMatrix2D(center, 45, 1.0)\nrotated = cv2.warpAffine(img, M, (w, h))\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#63-flipping","title":"6.3 Flipping","text":"<pre><code>flip_horizontal = cv2.flip(img, 1)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#7-feature-detection","title":"7. Feature Detection","text":"","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#71-corner-detection","title":"7.1 Corner Detection","text":"<pre><code>gray = np.float32(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\ncorners = cv2.goodFeaturesToTrack(gray, 25, 0.01, 10)\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(img, (int(x), int(y)), 5, (0, 0, 255), -1)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#72-sift-scale-invariant-feature-transform","title":"7.2 SIFT (Scale-Invariant Feature Transform)","text":"<p>Requires <code>opencv-contrib-python</code>:</p> <pre><code>sift = cv2.SIFT_create()\nkeypoints, descriptors = sift.detectAndCompute(gray, None)\ncv2.drawKeypoints(img, keypoints, img)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#8-face-detection","title":"8. Face Detection","text":"<p>OpenCV includes Haar cascades for face and eye detection.</p> <pre><code>face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nfaces = face_cascade.detectMultiScale(gray, 1.3, 5)\n\nfor (x, y, w, h) in faces:\n    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#9-integration-with-deep-learning","title":"9. Integration with Deep Learning","text":"<p>OpenCV supports importing pre-trained neural networks via DNN (Deep Neural Network) module.</p> <p>You can load models from:</p> <ul> <li> <p>TensorFlow (<code>.pb</code>)</p> </li> <li> <p>Caffe (<code>.prototxt</code>, <code>.caffemodel</code>)</p> </li> <li> <p>ONNX (<code>.onnx</code>)</p> </li> </ul> <p>Example:</p> <pre><code>net = cv2.dnn.readNetFromONNX('model.onnx')\nblob = cv2.dnn.blobFromImage(img, 1/255.0, (224, 224))\nnet.setInput(blob)\noutput = net.forward()\n</code></pre>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#10-real-world-applications","title":"10. Real-World Applications","text":"Field Example Security Face recognition, surveillance cameras Healthcare Medical image segmentation Autonomous Vehicles Lane detection, object tracking Augmented Reality Marker tracking Robotics Machine vision for object manipulation Education Teaching visual computing concepts","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#11-example-real-time-edge-detection","title":"11. Example: Real-Time Edge Detection","text":"<pre><code>import cv2\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    edges = cv2.Canny(gray, 50, 150)\n\n    cv2.imshow('Edges', edges)\n\n    if cv2.waitKey(1) == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Press Q to quit the window.</p>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#12-performance-tips","title":"12. Performance Tips","text":"<p>\u2705 Convert images to grayscale early when color is unnecessary \u2705 Resize large images to smaller dimensions for real-time speed \u2705 Use ROI (region of interest) to limit processing area \u2705 Use GPU acceleration via OpenCV CUDA (if available) \u2705 Pre-load models to avoid re-initialization cost</p>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#13-integrating-opencv-with-other-tools","title":"13. Integrating OpenCV with Other Tools","text":"<ul> <li> <p>NumPy: for numerical operations on image arrays</p> </li> <li> <p>Matplotlib: for visualizing results in notebooks</p> </li> <li> <p>TensorFlow / PyTorch: for advanced neural networks</p> </li> <li> <p>Flask / FastAPI: for building image-processing web APIs</p> </li> <li> <p>Raspberry Pi: for embedded vision projects</p> </li> </ul>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#14-opencv-ecosystem","title":"14. OpenCV Ecosystem","text":"<ul> <li> <p>opencv-python \u2192 Core image/video operations</p> </li> <li> <p>opencv-contrib-python \u2192 Extra modules (SIFT, SURF, tracking, etc.)</p> </li> <li> <p>cvzone \u2192 Simplified wrapper for common tasks</p> </li> <li> <p>mediapipe \u2192 Google\u2019s library often used with OpenCV for pose or hand detection</p> </li> </ul>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/opencv/#15-conclusion","title":"15. Conclusion","text":"<p>OpenCV bridges the gap between raw image data and intelligent computer vision applications. It provides an efficient, open-source foundation for learning and building image processing, machine learning, and AI-based visual systems.</p> <p>Whether you\u2019re exploring basic image filters or implementing face detection, OpenCV is a must-have toolkit for anyone studying or practicing computer vision.</p> <p>Further Reading</p> <ul> <li> <p>Official OpenCV Documentation</p> </li> <li> <p>OpenCV-Python Tutorials</p> </li> <li> <p>OpenCV GitHub Repository</p> </li> <li> <p>PyImageSearch Blog</p> </li> </ul>","tags":["opencv","computer-vision","python","image-processing","cs101"]},{"location":"OpenCV/usage/","title":"OpenCV Common Usages","text":"","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#introduction","title":"Introduction","text":"<p>OpenCV (Open Source Computer Vision Library) is an open-source computer vision and image-processing library originally developed by Intel. It provides a comprehensive set of tools for real-time image processing, computer vision, and machine learning tasks.  </p> <p>OpenCV supports multiple languages \u2014 including Python, C++, and Java \u2014 and runs on various platforms such as Windows, macOS, Linux, and even mobile devices.</p> <p>This document introduces OpenCV\u2019s most commonly used functions and workflows to help you quickly get started with practical image and video processing tasks.</p>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#1-installation-and-import","title":"1. Installation and Import *","text":"<p>To install OpenCV for Python:</p> <pre><code>pip install opencv-python\npip install opencv-contrib-python\n</code></pre> <p>Import the library in your Python code:</p> <pre><code>import cv2\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#2-reading-and-displaying-images","title":"2. Reading and Displaying Images *","text":"<p>OpenCV uses the <code>cv2.imread()</code> and <code>cv2.imshow()</code> functions to read and display images.</p> <pre><code>import cv2\n\n# Read an image\nimg = cv2.imread('example.jpg')\n\n# Display the image in a window\ncv2.imshow('Display Window', img)\n\n# Wait for a key press\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre> <p>\ud83d\udca1 <code>cv2.waitKey(0)</code> means the program will wait indefinitely for a key press before closing the window.</p>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#3-image-properties-and-basic-info","title":"3. Image Properties and Basic Info *","text":"<p>You can easily access image dimensions, type, and pixel values.</p> <pre><code>print(img.shape)   # (height, width, channels)\nprint(img.size)    # Total number of pixels\nprint(img.dtype)   # Data type, e.g., uint8\n</code></pre> <p>To access individual pixel values:</p> <pre><code>px = img[100, 50]  # Get BGR value at (x=50, y=100)\nprint(px)\n</code></pre> <p>To modify pixels:</p> <pre><code>img[100, 50] = [255, 255, 255]  # Set to white\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#4-image-color-conversions","title":"4. Image Color Conversions","text":"<p>Color space conversion is fundamental in image processing.</p> <pre><code>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nhsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n</code></pre> <p>Common conversions:</p> <ul> <li> <p><code>cv2.COLOR_BGR2GRAY</code> \u2014 Convert to grayscale = 0</p> </li> <li> <p><code>cv2.COLOR_BGR2RGB</code> \u2014 Convert from BGR to RGB</p> </li> <li> <p><code>cv2.COLOR_BGR2HSV</code> \u2014 Convert to HSV color space</p> </li> </ul>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#5-image-resizing-and-cropping","title":"5. Image Resizing and Cropping","text":"","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#resizing","title":"Resizing:","text":"<pre><code>resized = cv2.resize(img, (400, 300))\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#cropping-via-numpy-slicing","title":"Cropping (via NumPy slicing):","text":"<pre><code>cropped = img[50:200, 100:400]\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#6-drawing-shapes-and-text","title":"6. Drawing Shapes and Text","text":"<p>OpenCV allows drawing geometric shapes and adding text to images.</p> <pre><code>cv2.line(img, (0, 0), (200, 200), (255, 0, 0), 3)\ncv2.rectangle(img, (50, 50), (250, 150), (0, 255, 0), 2)\ncv2.circle(img, (150, 100), 50, (0, 0, 255), -1)\ncv2.putText(img, 'OpenCV', (50, 250), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#7-image-thresholding","title":"7. Image Thresholding","text":"<p>Thresholding separates objects from the background by converting grayscale images into binary images.</p> <pre><code>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n_, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n</code></pre> <p>Common types:</p> <ul> <li> <p><code>THRESH_BINARY</code></p> </li> <li> <p><code>THRESH_BINARY_INV</code></p> </li> <li> <p><code>THRESH_TRUNC</code></p> </li> <li> <p><code>THRESH_TOZERO</code></p> </li> </ul>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#8-edge-detection","title":"8. Edge Detection","text":"","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#using-canny-edge-detector","title":"Using Canny Edge Detector:","text":"<pre><code>edges = cv2.Canny(img, 100, 200)\n</code></pre> <p>This detects edges based on gradient changes and thresholds.</p>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#9-image-blurring-and-filtering","title":"9. Image Blurring and Filtering","text":"<p>Blurring helps remove noise and smooth images.</p> <pre><code>blur = cv2.GaussianBlur(img, (5, 5), 0)\nmedian = cv2.medianBlur(img, 5)\nbilateral = cv2.bilateralFilter(img, 9, 75, 75)\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#10-contour-detection","title":"10. Contour Detection","text":"<p>Contours are useful for shape analysis and object detection.</p> <pre><code>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n_, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\ncontours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\ncv2.drawContours(img, contours, -1, (0, 255, 0), 2)\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#11-reading-and-writing-video","title":"11. Reading and Writing Video","text":"","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#reading-from-a-file-or-camera","title":"Reading from a file or camera:","text":"<pre><code>cap = cv2.VideoCapture('video.mp4')  # or 0 for webcam\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    cv2.imshow('Video', frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#writing-a-video","title":"Writing a video:","text":"<pre><code>out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 20, (640, 480))\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#12-face-detection-using-haar-cascades","title":"12. Face Detection (Using Haar Cascades)","text":"<p>OpenCV includes pretrained classifiers for face detection.</p> <pre><code>face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nfaces = face_cascade.detectMultiScale(gray, 1.1, 4)\n\nfor (x, y, w, h) in faces:\n    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#13-saving-images","title":"13. Saving Images","text":"<pre><code>cv2.imwrite('output.jpg', img)\n</code></pre>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#14-common-tips","title":"14. Common Tips","text":"<ul> <li> <p>OpenCV uses BGR color order, not RGB. Always convert if mixing with libraries like <code>matplotlib</code>.</p> </li> <li> <p>Use NumPy operations directly on images for speed.</p> </li> <li> <p>Always call <code>cv2.destroyAllWindows()</code> after displaying windows to release memory.</p> </li> </ul>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#15-summary","title":"15. Summary","text":"<p>OpenCV provides a powerful, flexible, and efficient toolkit for:</p> <ul> <li> <p>Image reading, manipulation, and filtering</p> </li> <li> <p>Object detection and tracking</p> </li> <li> <p>Video processing</p> </li> <li> <p>Real-time vision and machine learning</p> </li> </ul> <p>Mastering the common APIs listed here gives you a strong foundation for more advanced topics like deep learning-based computer vision, AR, or autonomous systems.</p>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/usage/#references","title":"References","text":"<ul> <li> <p>OpenCV Official Documentation</p> </li> <li> <p>OpenCV GitHub Repository</p> </li> <li> <p>OpenCV Python Tutorials</p> </li> </ul>","tags":["opencv","python","image-processing","ai"]},{"location":"OpenCV/advanced/Untitled%201/","title":"Untitled 1","text":"<pre><code>import cv2  \n\n# Create a CSRT tracker  \ntracker = cv2.TrackerCSRT_create()  \n\n# Tracking status flag  \ntracking = False  \n\n# Open webcam (0 = default camera)  \ncap = cv2.VideoCapture(0)  \n\nwhile True:  \n    # Read a video frame  \n    ret, frame = cap.read()  \n    if not ret:  \n        print(\"Error: Cannot read frame from camera.\")  \n        break  \n\n    # Press 's' to select an ROI and start tracking  \n    if cv2.waitKey(1) == ord('s'):  \n        tracking = True  \n\n        # Select the ROI on the current frame  \n        roi = cv2.selectROI(  \n            windowName='Tracking',  \n            img=frame,  \n            showCrosshair=False  \n        )  \n\n        # Initialize the tracker with the selected ROI  \n        tracker.init(frame, roi)  \n\n    # If tracking is active, update the tracker on the new frame  \n    if tracking:  \n        success, box = tracker.update(frame)  \n\n        # If tracking succeeded, draw the bounding box  \n        if success:  \n            x, y, w, h = [int(v) for v in box]   # Ensure all values are ints  \n            cv2.rectangle(  \n                frame,  \n                pt1=(x, y),  \n                pt2=(x + w, y + h),  \n                color=(0, 255, 0),  \n                thickness=2  \n            )  \n        else:  \n            cv2.putText(  \n                frame,  \n                \"Tracking failed!\",  \n                (20, 40),  \n                cv2.FONT_HERSHEY_SIMPLEX,  \n                1.0,  \n                (0, 0, 255),  \n                2  \n            )  \n\n    # Display the result frame  \n    cv2.imshow('Tracking', frame)  \n\n    # Press ESC (key code 27) to exit  \n    if cv2.waitKey(1) == 27:  \n        break  \n\n# Release camera and close windows  \ncap.release()  \ncv2.destroyAllWindows()\n</code></pre>"},{"location":"OpenCV/advanced/Untitled/","title":"Untitled","text":"<p>\u4e0b\u9762\u7ed9\u51fa\u4e00\u4e2a\u6e05\u6670\u3001\u53ef\u76f4\u63a5\u8fd0\u884c\u7684 OpenCV \u6a21\u677f\u5339\u914d\u793a\u4f8b\u4ee3\u7801\uff08Python\uff09\uff0c\u5e76\u5305\u542b\u82f1\u6587\u6ce8\u91ca\u3002\u4ee3\u7801\u793a\u4f8b\u540c\u65f6\u5c55\u793a\u5982\u4f55\u53ef\u89c6\u5316\u5339\u914d\u7ed3\u679c\uff0c\u5e76\u652f\u6301\u591a\u79cd\u5339\u914d\u65b9\u6cd5\u3002</p>"},{"location":"OpenCV/advanced/Untitled/#opencv-template-matching-example-python","title":"\u2705 OpenCV Template Matching Example (Python)","text":"<pre><code>import cv2  \n\n# Load images  \n# source_img: the large image where we search  \n# template_img: the small image (template)  \nsource_img = cv2.imread('cola-bottle.png')  \ntemplate_img = cv2.imread('cola-logo.png')  \n\nif source_img is None or template_img is None:  \n    raise FileNotFoundError(\"Failed to load images. Check file paths.\")  \n\n# Convert to grayscale for template matching  \nsource_gray = cv2.cvtColor(source_img, cv2.COLOR_BGR2GRAY)  \ntemplate_gray = cv2.cvtColor(template_img, cv2.COLOR_BGR2GRAY)  \n\n# Template size  \nh, w = template_gray.shape[:2]  \n\n# Apply template matching  \n# Method options:  \n#   cv2.TM_CCOEFF_NORMED  (most commonly used)  \n#   cv2.TM_SQDIFF         (lower value = better match)  \n#   cv2.TM_CCORR_NORMED   etc.  \nmethod = cv2.TM_CCOEFF_NORMED  \n\nresult = cv2.matchTemplate(source_gray, template_gray, method)  \n\n# Locate the best match  \n# For TM_SQDIFF family, minimum is the best match.  \nmin_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)  \n\nif method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:  \n    top_left = min_loc  \nelse:  \n    top_left = max_loc  \n\nbottom_right = (top_left[0] + w, top_left[1] + h)  \n\n# Draw rectangle on result  \noutput = source_img.copy()  \ncv2.rectangle(output, top_left, bottom_right, (0, 255, 0), 2)  \n\n# Show results  \ncv2.imshow('Source Image', source_img)  \ncv2.imshow('Template', template_img)  \ncv2.imshow('Matched Result', output)  \ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n</code></pre>"},{"location":"OpenCV/advanced/Untitled/#how-it-works","title":"\ud83d\udccc How It Works","text":"<ol> <li> <p>\u52a0\u8f7d\u539f\u56fe\u548c\u6a21\u677f\u56fe\u3002</p> </li> <li> <p>\u7edf\u4e00\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\uff08\u6a21\u677f\u5339\u914d\u901a\u5e38\u5728\u7070\u5ea6\u7a7a\u95f4\u8fdb\u884c\uff09\u3002</p> </li> <li> <p>\u4f7f\u7528 <code>cv2.matchTemplate()</code> \u8fdb\u884c\u5339\u914d\u3002</p> </li> <li> <p>\u4f7f\u7528 <code>cv2.minMaxLoc()</code> \u627e\u5230\u6700\u4f73\u5339\u914d\u4f4d\u7f6e\u3002</p> </li> <li> <p>\u5728\u539f\u56fe\u4e0a\u7ed8\u5236\u77e9\u5f62\u6846\u3002</p> </li> </ol>"},{"location":"OpenCV/advanced/Untitled/#_1","title":"\u2b50 \u5e38\u7528\u5339\u914d\u65b9\u6cd5\u5bf9\u6bd4","text":"Method Meaning Good Match <code>TM_CCOEFF_NORMED</code> \u76f8\u5173\u7cfb\u6570 \u6570\u503c\u8d8a\u5927\u8d8a\u597d <code>TM_CCORR_NORMED</code> \u4e92\u76f8\u5173 \u6570\u503c\u8d8a\u5927\u8d8a\u597d <code>TM_SQDIFF</code> \u5dee\u503c \u6570\u503c\u8d8a\u5c0f\u8d8a\u597d <code>TM_SQDIFF_NORMED</code> \u5dee\u503c\u5f52\u4e00\u5316 \u6570\u503c\u8d8a\u5c0f\u8d8a\u597d <p>\u5982\u6709\u9700\u8981\uff0c\u6211\u4e5f\u53ef\u4ee5\u7ed9\u4f60\uff1a \u2705 \u5728\u89c6\u9891\u4e2d\u8fdb\u884c\u6a21\u677f\u5339\u914d\u7684\u7248\u672c \u2705 \u591a\u76ee\u6807\u5339\u914d\uff08\u627e\u5230\u6240\u6709\u5339\u914d\u4f4d\u7f6e\uff09 \u2705 \u5e26\u9608\u503c\u7684\u5339\u914d\u8fc7\u6ee4 \u2705 \u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u6027\u80fd\u4f18\u5316\u5efa\u8bae</p> <p>\u968f\u65f6\u544a\u8bc9\u6211\u4f60\u60f3\u8fdb\u4e00\u6b65\u6269\u5c55\u54ea\u90e8\u5206\uff01</p>"},{"location":"OpenCV/advanced/support-chinese/","title":"\ud83d\udcd8 How to Enable Chinese Text in OpenCV (with Example)","text":"<p>OpenCV\u2019s built-in <code>cv2.putText()</code> does not support Chinese characters because it only uses a limited Hershey font. To display Chinese text, you must:</p> <ol> <li> <p>Load the image using OpenCV</p> </li> <li> <p>Convert it to a PIL image</p> </li> <li> <p>Use a TrueType Chinese font (<code>.ttf</code>) to draw Chinese text</p> </li> <li> <p>Convert back to an OpenCV <code>numpy</code> array</p> </li> <li> <p>Continue using OpenCV normally</p> </li> </ol>"},{"location":"OpenCV/advanced/support-chinese/#requirements","title":"\u2705 Requirements","text":"<p>Install Pillow (if not already installed):</p> <pre><code>pip install pillow\n</code></pre> <p>Prepare a font that supports Chinese, such as:</p> <ul> <li> <p><code>SimHei.ttf</code> (\u9ed1\u4f53)</p> </li> <li> <p><code>SimSun.ttf</code> (\u5b8b\u4f53)</p> </li> <li> <p><code>NotoSansCJK-Regular.ttc</code> (Google Noto font)</p> </li> </ul> <p>Place the font file in the same directory or specify a full path.</p>"},{"location":"OpenCV/advanced/support-chinese/#full-example-code","title":"\u2705 Full Example Code","text":"<pre><code>import cv2\nfrom PIL import Image, ImageDraw, ImageFont\nimport numpy as np\n\n# --- Load image using OpenCV ---\nimg = cv2.imread(\"test.jpg\")\n\n# Convert cv2 image (BGR) to PIL image (RGB)\nimg_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n\n# Create draw object\ndraw = ImageDraw.Draw(img_pil)\n\n# Load a Chinese font file\nfont = ImageFont.truetype(\"SimHei.ttf\", 40)\n\n# Text to draw\ntext = \"\u4f60\u597d\uff0cOpenCV\uff01\"\n\n# Draw Chinese text\ndraw.text((50, 50), text, font=font, fill=(255, 0, 0))  # red text\n\n# Convert back to OpenCV (BGR)\nimg = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n\n# Display\ncv2.imshow(\"Image with Chinese\", img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"OpenCV/advanced/support-chinese/#explanation","title":"\u2705 Explanation","text":""},{"location":"OpenCV/advanced/support-chinese/#why-opencv-cannot-display-chinese","title":"Why OpenCV cannot display Chinese","text":"<p>OpenCV uses Hershey vector fonts which only support ASCII characters. Any Chinese characters will appear as:</p> <ul> <li> <p>empty boxes</p> </li> <li> <p>question marks</p> </li> <li> <p>nothing displayed</p> </li> </ul>"},{"location":"OpenCV/advanced/support-chinese/#why-pil-works","title":"Why PIL works","text":"<p>Pillow supports any <code>.ttf</code> or <code>.ttc</code> TrueType font, so it can accurately render:</p> <ul> <li> <p>Chinese</p> </li> <li> <p>Japanese</p> </li> <li> <p>Korean</p> </li> <li> <p>Emoji</p> </li> <li> <p>Any Unicode characters</p> </li> </ul>"},{"location":"OpenCV/advanced/support-chinese/#notes","title":"\ud83d\udccc Notes","text":"<ul> <li> <p>The font file must support Chinese characters.</p> </li> <li> <p>If colors look wrong (red \u2194 blue), it\u2019s because OpenCV uses BGR, PIL uses RGB.</p> </li> <li> <p>For real-time webcam overlay, you can repeat the PIL conversion inside your loop.</p> </li> </ul>"},{"location":"OpenCV/advanced/support-chinese/#quick-reference","title":"\ud83d\udcd8 Quick Reference","text":"Task Method Display English text only <code>cv2.putText()</code> Display Chinese text Use Pillow (<code>ImageDraw.text</code>) Fonts needed <code>.ttf</code> or <code>.ttc</code> that supports Chinese Color mismatch Convert between BGR \u2194 RGB"},{"location":"OpenCV/basics/ROI/","title":"ROI (Region of Interest)","text":"<p>In image processing and computer vision, we often do not need to analyze the entire image \u2014 instead, we focus on a specific part that matters, such as a face, an object, or a block of text. This focused area is called the ROI, or Region of Interest.</p>","tags":["ROI"]},{"location":"OpenCV/basics/ROI/#what-is-roi","title":"\ud83e\udde0 What Is ROI?","text":"<p>ROI stands for Region of Interest. It refers to a specific area within an image or video that you want to analyze, detect, recognize, or process further.</p> <p>An ROI can be:</p> <ul> <li> <p>Manually selected by the user (e.g., via coordinate slicing), or</p> </li> <li> <p>Automatically detected by algorithms (e.g., face detection, object tracking, OCR, etc.).</p> </li> </ul>","tags":["ROI"]},{"location":"OpenCV/basics/ROI/#extracting-roi-in-opencv","title":"\ud83e\udde9 Extracting ROI in OpenCV","text":"<p>In OpenCV, images are stored as NumPy arrays. That means we can easily use array slicing to extract any rectangular area as an ROI.</p> <pre><code>import cv2  \n\n# Read an image  \nimg = cv2.imread('ex.jpg')  \n\n# Extract ROI (rows 30\u2013230, columns 100\u2013300)  \nroi = img[30:230, 100:300]  \n\n# Display the original image and ROI  \ncv2.imshow('Original', img)  \ncv2.imshow('ROI', roi)  \n\ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n</code></pre>","tags":["ROI"]},{"location":"OpenCV/basics/ROI/#code-explanation","title":"\ud83d\udd0d Code Explanation","text":"Code Description <code>a = cv2.imread('timg98.jpg')</code> Reads the image as a NumPy array. <code>b = a[30:230, 100:300]</code> Extracts a rectangular ROI using NumPy slicing. <code>cv2.imshow('yuantu', a)</code> Displays the original image. <code>cv2.imshow('qiepian', b)</code> Displays the extracted ROI. <code>cv2.waitKey(0)</code> Waits for a key press indefinitely. <code>cv2.destroyAllWindows()</code> Closes all image display windows.","tags":["ROI"]},{"location":"OpenCV/basics/ROI/#coordinate-system","title":"\ud83d\udcd0 Coordinate System","text":"<p>In OpenCV, image coordinates follow the same convention as NumPy arrays:</p> <ul> <li> <p>First index (rows) \u2192 y-axis (vertical)</p> </li> <li> <p>Second index (columns) \u2192 x-axis (horizontal)</p> </li> </ul> <p>Thus:</p> <pre><code>roi = img[y1:y2, x1:x2]\n</code></pre> <p>where:</p> <ul> <li> <p><code>(x1, y1)</code> is the top-left corner, and</p> </li> <li> <p><code>(x2, y2)</code> is the bottom-right corner.</p> </li> </ul>","tags":["ROI"]},{"location":"OpenCV/basics/ROI/#common-use-cases","title":"\u2699\ufe0f Common Use Cases","text":"<p>ROIs are widely used in many computer vision tasks, such as:</p> <ul> <li> <p>Focusing analysis on specific regions</p> </li> <li> <p>Extracting candidate areas in object detection</p> </li> <li> <p>Tracking moving objects in video streams</p> </li> <li> <p>Performing OCR (text recognition) within detected text regions</p> </li> <li> <p>Reducing computation time and memory usage</p> </li> </ul>","tags":["ROI"]},{"location":"OpenCV/basics/ROI/#tips","title":"\ud83d\udca1 Tips","text":"<ul> <li> <p>ROI slicing in OpenCV does not create a new image copy \u2014 it returns a view of the original array.     Any modification to the ROI will also affect the original image.</p> </li> <li> <p>To create an independent copy, use:</p> <pre><code>roi = a[30:230, 100:300].copy()\n</code></pre> </li> </ul>","tags":["ROI"]},{"location":"OpenCV/basics/ROI/#further-reading","title":"\ud83d\udcda Further Reading","text":"<ul> <li> <p>OpenCV Docs \u2014 Basic Operations on Images</p> </li> <li> <p>NumPy Array Indexing</p> </li> <li> <p>What Is ROI in Computer Vision (Medium)</p> </li> </ul>","tags":["ROI"]},{"location":"OpenCV/basics/ROI/#summary","title":"\u2705 Summary","text":"<p>The Region of Interest (ROI) is a fundamental concept in image analysis. By using NumPy slicing, you can quickly isolate and manipulate key regions in your images \u2014 a crucial step for object detection, recognition, and visual understanding in computer vision workflows.</p>","tags":["ROI"]},{"location":"OpenCV/basics/image_operations/","title":"Image Operations in OpenCV","text":"<p>This section introduces several essential image operations in OpenCV, including:</p> <ul> <li> <p>Modifying image pixels</p> </li> <li> <p>Combining images</p> </li> <li> <p>Resizing images</p> </li> </ul> <p>All examples are based on NumPy and OpenCV (<code>cv2</code>).</p>","tags":["image"]},{"location":"OpenCV/basics/image_operations/#1-modifying-image-pixels","title":"\ud83e\udde0 1. Modifying Image Pixels","text":"<p>In OpenCV, images are stored as NumPy arrays \u2014 so you can directly access and modify their pixel values using slicing.</p> <pre><code>import cv2  \nimport numpy as np  \n\n# Read the image  \nimg = cv2.imread('ex.jpg')  \n\n# Replace a region with random color values  \nimg[0:200, 0:200] = np.random.randint(0, 256, (200, 200, 3))  \n\ncv2.imshow('Mosaic', img)  \ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n</code></pre>","tags":["image"]},{"location":"OpenCV/basics/image_operations/#explanation","title":"Explanation","text":"Code Description <code>np.random.randint(0, 256, (100, 100, 3))</code> Generates a random RGB color matrix of size 100\u00d7100. <code>a[100:200, 200:300] = ...</code> Replaces that area in the original image with the random colors. <code>cv2.imshow()</code> Displays the modified image. <p>\u26a0\ufe0f Note: The replacement area and the array size must match exactly \u2014 otherwise, NumPy will raise a shape error.</p>","tags":["image"]},{"location":"OpenCV/basics/image_operations/#2-combining-two-images","title":"\ud83e\udde9 2. Combining Two Images","text":"<p>We can also paste one image onto another \u2014 for example, inserting a smaller image into a specific region of a larger one.</p> <pre><code>import cv2  \n\nimg_a = cv2.imread('ex.jpg')        # Background image  \nimg_b = cv2.imread('avatar.jpg')    # Foreground image  \n\n# Define a region in the background and copy the smaller image into it  \nimg_a[200:600, 200:600] = img_b[100:500, 200:600]  \n\ncv2.imshow('Image A', img_a)  \ncv2.imshow('Image B', img_b)  \ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n</code></pre>","tags":["image"]},{"location":"OpenCV/basics/image_operations/#explanation_1","title":"Explanation","text":"Concept Description Image overlay A smaller image <code>b</code> is copied into a region of <code>a</code>. Same size requirement The region of <code>a</code> and the part of <code>b</code> must have the same dimensions. Practical use This technique can be used for watermarking, object blending, or image mosaics.","tags":["image"]},{"location":"OpenCV/basics/image_operations/#3-resizing-images-with-cv2resize","title":"\ud83d\uddbc\ufe0f 3. Resizing Images with <code>cv2.resize</code>","text":"<p><code>cv2.resize()</code> is used to scale images up or down. It accepts several parameters to control size and scaling behavior.</p> <pre><code>import cv2  \n\nimg = cv2.imread('ex.jpg')  \n\n# Resize the image to a fixed width and height  \nresized_img = cv2.resize(img, (100, 200))  \n# resized_img = cv2.resize(img, dsize=None, fx=0.5, fy=0.5)  \n\ncv2.imshow('Image', img)  \ncv2.imshow('Resized Image', resized_img)  \ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n</code></pre>","tags":["image"]},{"location":"OpenCV/basics/image_operations/#parameters","title":"Parameters","text":"Parameter Description <code>src</code> Source image (NumPy array). <code>dsize</code> Target size (width, height). If <code>None</code>, scaling factors <code>fx</code> and <code>fy</code> are used instead. <code>fx</code>, <code>fy</code> Scaling factors for the x and y axes. <code>interpolation</code> (Optional) Interpolation method, e.g., <code>cv2.INTER_LINEAR</code>, <code>cv2.INTER_AREA</code>, etc. <p>Example with scale factors:</p> <pre><code>a_new = cv2.resize(a, dsize=None, fx=0.5, fy=0.5)\n</code></pre>","tags":["image"]},{"location":"OpenCV/basics/image_operations/#practical-notes","title":"\u2699\ufe0f Practical Notes","text":"<ul> <li> <p>Upscaling vs Downscaling:</p> <ul> <li> <p>For shrinking, <code>cv2.INTER_AREA</code> works best.</p> </li> <li> <p>For enlarging, use <code>cv2.INTER_LINEAR</code> or <code>cv2.INTER_CUBIC</code>.</p> </li> </ul> </li> <li> <p>Aspect ratio:     Always keep the same aspect ratio unless a specific deformation is desired.</p> </li> <li> <p>Performance:     Resizing large images repeatedly can be expensive; cache or precompute resized versions if possible.</p> </li> </ul>","tags":["image"]},{"location":"OpenCV/basics/image_operations/#summary","title":"\u2705 Summary","text":"Operation Function Description Modify pixels NumPy slicing Change specific image areas Combine images Image assignment Paste one image into another Resize image <code>cv2.resize()</code> Scale images up or down <p>These techniques are fundamental for image preprocessing in computer vision \u2014 they are often used before tasks like feature extraction, segmentation, or model inference.</p>","tags":["image"]},{"location":"OpenCV/basics/img_add/","title":"Image Arithmetic Operations","text":"<pre><code>import cv2  \n\n# Read images  \nimg_a = cv2.imread('ex.jpg')  \nimg_b = cv2.imread('avatar.jpg')  \n\n# Simple pixel addition  \n# Adds 10 to all pixel values; values above 255 will wrap around (e.g. 260 -&gt; 4)  \nimg_c = img_a + 100  \n\n# cv2.imshow('Original', img_a)  \ncv2.imshow('After + 100', img_c)  \ncv2.waitKey(0)  \n\n# Add two image regions  \nroi_a = img_a[50:450, 50:400]  \nroi_b = img_b[50:450, 50:400]  \nroi_c = roi_a + roi_b  \n\ncv2.imshow('ROI Addition', roi_c)  \ncv2.waitKey(0)  \n\n# Safe addition using cv2.add()  \n# Ensures that overflow values are clipped to 255 instead of wrapping around.  \nresized_a = cv2.resize(img_a, (400, 400))  \nresized_b = cv2.resize(img_b, (400, 400))  \nresized_c = cv2.add(resized_a, resized_b)  \n\ncv2.imshow('Using cv2.add()', resized_c)  \ncv2.waitKey(0)  \n\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2  \n\n# Read and resize images  \nimg_a = cv2.imread('ex.jpg')  \nimg_b = cv2.imread('avatar.jpg')  \n\nresized_a = cv2.resize(img_a, (400, 400))  \nresized_b = cv2.resize(img_b, (400, 400))  \n\n# Weighted addition  \nweighted_img = cv2.addWeighted(resized_a, 0.5, resized_b, 0.5, 10)  \n\ncv2.imshow('Weighted Addition', weighted_img)  \ncv2.waitKey(0)  \ncv2.destroyAllWindows()\n</code></pre>"},{"location":"OpenCV/basics/img_add/#optimized-code","title":"Optimized Code","text":"<pre><code>\"\"\"\n-------------------------------------------------------\nImage Arithmetic Operations using OpenCV\n-------------------------------------------------------\nDemonstrates:\n1. Simple pixel addition\n2. Safe addition with cv2.add()\n3. Weighted image blending\n-------------------------------------------------------\nAuthor: AI101\n-------------------------------------------------------\n\"\"\"\n\nimport cv2\n\n\ndef show_image(window_name: str, image):\n    \"\"\"Display an image until a key is pressed.\"\"\"\n    cv2.imshow(window_name, image)\n    cv2.waitKey(0)\n\n\ndef add_scalar_to_image(image, value: int):\n    \"\"\"\n    Add a scalar value to all pixels in an image.\n    Note: Overflow wraps around (e.g., 260 \u2192 4).\n    \"\"\"\n    return image + value\n\n\ndef add_image_regions(img_a, img_b, region_a, region_b):\n    \"\"\"\n    Add two image regions directly (risk of overflow).\n    \"\"\"\n    roi_a = img_a[region_a[0]:region_a[1], region_a[2]:region_a[3]]\n    roi_b = img_b[region_b[0]:region_b[1], region_b[2]:region_b[3]]\n    return roi_a + roi_b\n\n\ndef safe_add_images(img_a, img_b, size=(400, 400)):\n    \"\"\"\n    Safely add two resized images using cv2.add(),\n    ensuring overflow values are clipped to 255.\n    \"\"\"\n    a_resized = cv2.resize(img_a, size)\n    b_resized = cv2.resize(img_b, size)\n    return cv2.add(a_resized, b_resized)\n\n\ndef weighted_blend(img_a, img_b, alpha=0.5, beta=0.5, gamma=10, size=(400, 400)):\n    \"\"\"\n    Blend two images using weighted addition:\n    result = \u03b1\u00b7A + \u03b2\u00b7B + \u03b3\n    \"\"\"\n    a_resized = cv2.resize(img_a, size)\n    b_resized = cv2.resize(img_b, size)\n    return cv2.addWeighted(a_resized, alpha, b_resized, beta, gamma)\n\n\ndef main():\n    # --- Load Images ---\n    img_a = cv2.imread(\"ex.jpg\")\n    img_b = cv2.imread(\"avatar.jpg\")\n\n    if img_a is None or img_b is None:\n        print(\"Error: One or both image files could not be loaded.\")\n        return\n\n    # --- Simple pixel addition ---\n    img_plus_100 = add_scalar_to_image(img_a, 100)\n    show_image(\"Original\", img_a)\n    show_image(\"After +100\", img_plus_100)\n\n    # --- Add two image regions (unsafe addition) ---\n    roi_sum = add_image_regions(img_a, img_b, (50, 450, 50, 400), (50, 450, 50, 400))\n    show_image(\"ROI Addition\", roi_sum)\n\n    # --- Safe addition using cv2.add() ---\n    safe_sum = safe_add_images(img_a, img_b)\n    show_image(\"cv2.add() Safe Addition\", safe_sum)\n\n    # --- Weighted image blending ---\n    blended = weighted_blend(img_a, img_b, alpha=0.5, beta=0.5, gamma=10)\n    show_image(\"Weighted Blend\", blended)\n\n    cv2.destroyAllWindows()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"OpenCV/basics/img_add/#ai101-documentation","title":"\ud83d\udcd8 AI101 Documentation","text":""},{"location":"OpenCV/basics/img_add/#topic-image-arithmetic-operations-in-opencv","title":"Topic: Image Arithmetic Operations in OpenCV","text":""},{"location":"OpenCV/basics/img_add/#overview","title":"Overview","text":"<p>Image arithmetic is a fundamental concept in computer vision. It involves pixel-wise mathematical operations on images, enabling effects such as brightness adjustment, image blending, and combining multiple image regions.</p> <p>OpenCV provides efficient built-in functions to perform arithmetic operations with automatic handling of overflow and type consistency.</p>"},{"location":"OpenCV/basics/img_add/#1-simple-pixel-addition","title":"1. Simple Pixel Addition","text":"<p>Operation:</p> <pre><code>img_result = img + 100\n</code></pre> <p>This adds a constant value to every pixel. \u26a0\ufe0f Caution: When using NumPy addition, pixel values above 255 wrap around (e.g., 260 \u2192 4).</p> <p>Use case: quick brightness enhancement (but unsafe).</p>"},{"location":"OpenCV/basics/img_add/#2-safe-addition-with-cv2add","title":"2. Safe Addition with <code>cv2.add()</code>","text":"<p>Operation:</p> <pre><code>result = cv2.add(imageA, imageB)\n</code></pre> <p>OpenCV automatically clips overflow values to 255, ensuring image data integrity.</p> <p>Use case: overlaying two images safely.</p>"},{"location":"OpenCV/basics/img_add/#3-region-addition","title":"3. Region Addition","text":"<p>You can perform arithmetic only on specific parts (Regions of Interest, ROI):</p> <pre><code>roi_a = img_a[y1:y2, x1:x2]\nroi_b = img_b[y1:y2, x1:x2]\nresult = roi_a + roi_b\n</code></pre> <p>Use case: combining local patches, partial overlays.</p>"},{"location":"OpenCV/basics/img_add/#4-weighted-image-blending","title":"4. Weighted Image Blending","text":"<p>Formula: [ \\text{result} = \\alpha \\cdot A + \\beta \\cdot B + \\gamma ]</p> <p>Implementation:</p> <pre><code>result = cv2.addWeighted(imageA, 0.5, imageB, 0.5, 10)\n</code></pre> <p>This produces a smooth, visually pleasing blend between two images \u2014 commonly used in crossfades, transparency effects, or HDR image composition.</p>"},{"location":"OpenCV/basics/img_add/#5-key-differences","title":"5. Key Differences","text":"Method Handles Overflow Adjustable Weights Typical Use <code>img + value</code> \u274c Wraps around \u274c Quick brightness change <code>cv2.add()</code> \u2705 Clipped at 255 \u274c Safe image addition <code>cv2.addWeighted()</code> \u2705 Clipped at 255 \u2705 Image blending"},{"location":"OpenCV/basics/img_add/#example-output-preview","title":"Example Output Preview","text":"Operation Result Description After +100 Brighter image (unsafe overflow) ROI Addition Local sum of two cropped areas cv2.add() Safe Addition Combined image without overflow artifacts Weighted Blend Smooth composite between both inputs"},{"location":"OpenCV/basics/img_add/#best-practices","title":"Best Practices","text":"<ul> <li> <p>Always check if images are loaded successfully.</p> </li> <li> <p>Use <code>cv2.add()</code> or <code>cv2.addWeighted()</code> for production-level image arithmetic.</p> </li> <li> <p>Resize images to the same size before combining.</p> </li> <li> <p>Use small gamma values (e.g. <code>+10</code>) for natural-looking brightness adjustments.</p> </li> </ul> <p>Would you like me to make this AI101 doc version include a Quick Reference Table + Code Snippets (like the \u201cExample + Output\u201d section in your previous AI101 docs)? That version fits perfectly for your documentation site.</p>"},{"location":"OpenCV/basics/img_add/#improvements-made","title":"\ud83d\udd27 Improvements Made","text":"<ul> <li> <p>Cleaned up redundant reads and redefinitions.</p> </li> <li> <p>Added clear variable naming (<code>roi_a</code>, <code>roi_b</code>, <code>a_resized</code>).</p> </li> <li> <p>Included comments about overflow vs. clipping behavior.</p> </li> <li> <p>Added consistent window names for easier comparison.</p> </li> </ul>"},{"location":"OpenCV/basics/img_add/#quick-reference-image-arithmetic-in-opencv","title":"\ud83e\uddfe Quick Reference: Image Arithmetic in OpenCV","text":""},{"location":"OpenCV/basics/img_add/#1-pixel-addition","title":"\ud83d\udd39 1. Pixel Addition","text":"<p>You can directly add a constant value to an image:</p> <pre><code>img_new = img + 10\n</code></pre> <p>\u26a0\ufe0f NumPy addition wraps around when the result exceeds 255. Example: <code>250 + 10 = 4</code>.</p>"},{"location":"OpenCV/basics/img_add/#2-image-addition-numpy","title":"\ud83d\udd39 2. Image Addition (NumPy)","text":"<pre><code>result = img1 + img2\n</code></pre> <p>Both images must be of the same size and data type. Overflow values wrap around \u2014 so <code>255 + 10 \u2192 9</code>.</p>"},{"location":"OpenCV/basics/img_add/#3-safe-addition-cv2add","title":"\ud83d\udd39 3. Safe Addition (<code>cv2.add</code>)","text":"<pre><code>result = cv2.add(img1, img2)\n</code></pre> <ul> <li> <p>Values are clipped at 255 instead of wrapping.     Example: <code>250 + 10 = 255</code></p> </li> <li> <p>Recommended for image blending or brightness adjustment.</p> </li> </ul>"},{"location":"OpenCV/basics/img_add/#4-weighted-addition-image-blending","title":"\ud83d\udd39 4. Weighted Addition (Image Blending)","text":"<p>Combine two images with given weights:</p> <pre><code>blend = cv2.addWeighted(img1, 0.6, img2, 0.4, 0)\n</code></pre> <p>Formula: [ \\text{output} = img1 \\times \\alpha + img2 \\times \\beta + \\gamma ]</p>"},{"location":"OpenCV/basics/img_add/#summary","title":"\ud83d\udcda Summary","text":"Operation Function Overflow Behavior Use Case <code>a + 10</code> NumPy add Wraps (260\u21924) Quick pixel test <code>a + b</code> NumPy add Wraps Simple math ops <code>cv2.add(a, b)</code> OpenCV add Clipped (260\u2192255) Safe blending <code>cv2.addWeighted()</code> Weighted add Clipped Image overlay / transparency"},{"location":"OpenCV/basics/read-image/","title":"\ud83d\udcf8 Reading an Image with <code>cv2.imread()</code>","text":"<p>The <code>cv2.imread()</code> function loads an image from the specified file path and returns it as a NumPy array.</p>","tags":["opencv","read","image"]},{"location":"OpenCV/basics/read-image/#1-prerequisites","title":"1. Prerequisites","text":"<p>You must have OpenCV (the Python library is typically <code>opencv-python</code>) and NumPy installed in your environment.</p> <pre><code>pip install opencv-python numpy\n</code></pre>","tags":["opencv","read","image"]},{"location":"OpenCV/basics/read-image/#2-syntax","title":"2. Syntax","text":"<p>The function takes two main arguments:</p> <ul> <li><code>filename</code> (string): The full path to the image file you want to read.</li> <li><code>flags</code> (integer, optional): Specifies how the image should be read. This is a crucial argument for controlling the output format.</li> </ul> <pre><code>import cv2\nimport numpy as np\n\n# Syntax:\nimage_array = cv2.imread(filename, flags)\n</code></pre>","tags":["opencv","read","image"]},{"location":"OpenCV/basics/read-image/#3-key-flags-explained","title":"3. Key <code>flags</code> Explained","text":"<p>The <code>flags</code> argument determines the color depth and transparency of the loaded image. It can take one of three main values:</p> Flag Name Value Description Output Channels <code>cv2.IMREAD_COLOR</code> <code>1</code> Loads a color image. Any transparency (alpha channel) is discarded. This is the default behavior. 3 (Blue, Green, Red) <code>cv2.IMREAD_GRAYSCALE</code> <code>0</code> Loads the image as a single-channel grayscale image. 1 (Intensity/Luminance) <code>cv2.IMREAD_UNCHANGED</code> <code>-1</code> Loads the image as is, including the alpha channel (transparency) if present. 4 (Blue, Green, Red, Alpha)","tags":["opencv","read","image"]},{"location":"OpenCV/basics/read-image/#4-detailed-example","title":"4. Detailed Example","text":"<p>This example demonstrates reading an image in all three modes and checking the resulting array shapes.</p> <pre><code>import cv2  \nimport numpy as np  \n\n# 1. Define the path to your image  \n# Make sure 'img.png' exists in the same directory, or use a full path.  \nimage_path = 'img.png'  \n\n# 2. Read the image in different modes  \n# ---  \n\n# A. Color Image (Default)  \nimg_color = cv2.imread(image_path, cv2.IMREAD_COLOR)  \nprint(f\"Color Image Shape: {img_color.shape}\")  \n# Expected output: (height, width, 3) e.g., (400, 600, 3)  \n\n# B. Grayscale Image  \nimg_gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  \nprint(f\"Grayscale Image Shape: {img_gray.shape}\")  \n# Expected output: (height, width) e.g., (400, 600) (Note: only 2 dimensions)  \n\n# C. Unchanged (with Alpha if available)  \nimg_unchanged = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  \nprint(f\"Unchanged Image Shape: {img_unchanged.shape}\")  \n# Expected output: (height, width, 4) if it has an alpha channel, or (height, width, 3)  \n\n# 3. Always Check for Successful Load  \nif img_color is None:  \n    print(f\"ERROR: Could not read image at {image_path}. Check the path.\")  \nelse:  \n    print(\"Image loaded successfully!\")  \n\n# 4. Display the image (optional but helpful)  \n# cv2.imshow('Color Image', img_color)  \n# cv2.waitKey(0)  \n# cv2.destroyAllWindows()\n</code></pre>","tags":["opencv","read","image"]},{"location":"OpenCV/basics/read-image/#5-important-details","title":"5. Important Details","text":"<ul> <li>Output Format: The image is returned as a NumPy array (specifically a <code>numpy.ndarray</code>). This allows for efficient numerical operations and integration with other Python libraries.</li> <li>Color Channel Order: OpenCV uses the Blue, Green, Red (BGR) channel order by default for color images, NOT the Red, Green, Blue (RGB) order commonly used in other applications. This is a critical detail for processing and displaying images.</li> <li>Failed Load: If the image file doesn't exist, the path is incorrect, or the file is corrupted, <code>cv2.imread()</code> will return <code>None</code>. You must check for this <code>None</code> value before attempting any operations on the image object.</li> <li>Pixel Values: The data type of the array is usually <code>uint8</code> (unsigned 8-bit integers), meaning pixel intensity values range from 0 (black) to 255 (white/full color).</li> </ul>","tags":["opencv","read","image"]},{"location":"OpenCV/basics/read_video/","title":"Read and Display a Video File","text":"<p>This example demonstrates how to read and display a video file frame by frame using OpenCV. It can also be adapted to display a live webcam feed by changing the video source.</p>","tags":["opencv","video","tutorial"]},{"location":"OpenCV/basics/read_video/#code-example","title":"Code Example","text":"<pre><code>\"\"\"\n-------------------------------------------------------\nVideo File Reading and Display using OpenCV\n-------------------------------------------------------\n\"\"\"\n\nimport cv2\n\n# Open the video file (replace with 0 for webcam)\nvideo_capture = cv2.VideoCapture('video.mp4')\n\n# Check if the video file was opened successfully\nif not video_capture.isOpened():\n    print(\"Error: Cannot open the video file.\")\n    exit()\n\n# Continuously read frames from the video\nwhile True:\n    ret, frame = video_capture.read()\n\n    # If reading fails (end of video), break the loop\n    if not ret:\n        print(\"End of video or failed to read frame.\")\n        break\n\n    # Convert the frame from BGR to grayscale (optional)\n    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Display the current frame\n    cv2.imshow('Video', frame_gray)\n\n    # Wait for 60 ms and check if the 'Esc' key (27) is pressed\n    if cv2.waitKey(60) == 27:\n        print(\"Video playback interrupted by user.\")\n        break\n\n# Release resources\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>","tags":["opencv","video","tutorial"]},{"location":"OpenCV/basics/read_video/#explanation","title":"Explanation","text":"<ul> <li><code>cv2.VideoCapture('video.mp4')</code> creates a video capture object to read the file.     Use <code>cv2.VideoCapture(0)</code> to open the default webcam.</li> <li><code>isOpened()</code> checks whether the file or device was successfully opened.</li> <li>The <code>while</code> loop continuously reads frames using <code>.read()</code>.<ul> <li><code>ret</code> indicates whether the frame was successfully read.</li> <li><code>frame</code> is the actual image array.</li> </ul> </li> <li><code>cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</code> converts the frame to grayscale.     This step is optional but often useful for processing.</li> <li><code>cv2.imshow()</code> displays the frame in a window.</li> <li><code>cv2.waitKey(60)</code> waits for 60 ms between frames;     pressing Esc (<code>key code 27</code>) stops playback.</li> <li><code>release()</code> and <code>destroyAllWindows()</code> properly close resources.</li> </ul>","tags":["opencv","video","tutorial"]},{"location":"OpenCV/basics/read_video/#notes","title":"Notes","text":"<ul> <li>Make sure the video file (<code>video.mp4</code>) is in the same directory as your script,     or provide an absolute path.</li> <li>To adjust playback speed, change the <code>cv2.waitKey()</code> delay (smaller = faster).</li> <li>If you only see a blank window, check whether the path to the video is correct.</li> <li>You can extend this script to record or process frames in real time.</li> </ul>","tags":["opencv","video","tutorial"]},{"location":"dlib/01-dlib-brief-intro/","title":"\ud83d\udc0d Dlib Library in Python: A Brief Introduction","text":"<p>The Dlib library in Python is a powerful wrapper around the core C++ Dlib toolkit. It provides a vast collection of machine learning algorithms, computer vision tools, and numerical optimization routines, all accessible through an easy-to-use Python interface.</p>"},{"location":"dlib/01-dlib-brief-intro/#key-areas-and-use-cases","title":"Key Areas and Use Cases","text":"<p>Dlib is most famously used in Python for Computer Vision (CV) and Machine Learning (ML) applications: 1. Face Recognition and Detection: Dlib excels in real-time facial processing. It provides highly accurate methods for:     - Face Detection: Locating faces in an image.     - Facial Landmark Detection: Pinpointing specific features on the face (eyes, nose, mouth) using models like the 68-point landmark predictor.     - Face Recognition: Generating a unique 128D face descriptor that can be used to compare and identify different faces. 2. Generic Object Detection: It includes tools for training high-performance object detectors using techniques like Support Vector Machine (SVM)-based detectors (e.g., the Histogram of Oriented Gradients or HOG method) and more recently, Deep Learning (DL) methods. 3. Machine Learning: Dlib offers a comprehensive suite of ML algorithms, including:     - Classification: SVMs, Logistic Regression, and DL models.     - Regression: Techniques for predicting continuous values.     - Clustering: Algorithms like K-Means and spectral clustering.     - Numerical Optimization: Robust tools for training complex models efficiently.</p>"},{"location":"dlib/01-dlib-brief-intro/#python-integration","title":"Python Integration","text":"<ul> <li>Easy Access: The Python bindings make the high-speed C++ algorithms accessible without needing to write C++ code.</li> <li>Performance: Because the heavy lifting is done by the optimized C++ backend, Dlib offers excellent performance for computationally intensive tasks like face processing.</li> <li>Installation: Dlib typically requires a C++ compiler to be installed on your system (like CMake) because it compiles the C++ source during the Python installation process.</li> </ul> <p>In summary, Dlib is an essential library for Python developers focused on high-performance, real-world computer vision and robust machine learning solutions.</p>"},{"location":"dlib/02-face-detection/","title":"Face Detection with dlib and OpenCV \u2014 Detailed Explanation","text":"<p>This document explains how the provided Python script performs face detection using dlib and OpenCV, describes the underlying technologies, and provides annotated comments to help beginners understand every step.</p>"},{"location":"dlib/02-face-detection/#1-full-code-with-comments","title":"1. Full Code with Comments","text":"<pre><code>import dlib\nimport cv2\n\n# Load dlib's default HOG + SVM face detector.\ndetector = dlib.get_frontal_face_detector()\n\n# Read an image from disk. Replace \"face.jpg\" with your own image file.\nimg = cv2.imread(\"face.jpg\")\n\n# Convert the image from BGR (OpenCV's default) to RGB (dlib's expected format).\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Detect faces in the image.\n# The second argument '2' is the upsampling number, which improves accuracy on small faces.\nfaces = detector(img_rgb, 2)\n\n# Loop through each detected face and draw a rectangle.\nfor face in faces:\n    x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n    cv2.rectangle(img, (x1, y1), (x2, y2), (0,255,0), 2)\n\n# Display the final result with bounding boxes.\ncv2.imshow(\"result\", img)\ncv2.waitKey(0)\n</code></pre>"},{"location":"dlib/02-face-detection/#2-step-by-step-explanation","title":"2. Step-by-Step Explanation","text":""},{"location":"dlib/02-face-detection/#21-importing-libraries","title":"2.1 Importing Libraries","text":"<pre><code>import dlib\nimport cv2\n</code></pre> <ul> <li><code>dlib</code>: Provides machine learning and computer vision tools, including its famous face detector.</li> <li><code>cv2</code>: OpenCV, used for image loading, color conversion, and visualization.</li> </ul>"},{"location":"dlib/02-face-detection/#22-loading-the-face-detector","title":"2.2 Loading the Face Detector","text":"<pre><code>detector = dlib.get_frontal_face_detector()\n</code></pre> <p>This loads dlib\u2019s built-in HOG + SVM face detector.</p> <ul> <li>HOG (Histogram of Oriented Gradients)     Used to extract gradient orientation features from the image.</li> <li>SVM (Support Vector Machine)     A classifier trained to distinguish faces from non-faces.</li> </ul> <p>This detector works well for frontal human faces and runs fully on CPU.</p>"},{"location":"dlib/02-face-detection/#23-reading-and-processing-the-image","title":"2.3 Reading and Processing the Image","text":"<pre><code>img = cv2.imread(\"face.jpg\")\n</code></pre> <p>Loads an image in BGR format, which is OpenCV\u2019s default channel order.</p> <pre><code>img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n</code></pre> <p>dlib expects RGB images, so we convert it.</p>"},{"location":"dlib/02-face-detection/#24-detecting-faces","title":"2.4 Detecting Faces","text":"<pre><code>faces = detector(img_rgb, 2)\n</code></pre>"},{"location":"dlib/02-face-detection/#the-return-value","title":"The return value:","text":"<p><code>faces</code> is a list of <code>dlib.rectangle</code> objects. Each rectangle represents one detected face.</p>"},{"location":"dlib/02-face-detection/#the-meaning-of-the-second-argument-2","title":"The meaning of the second argument (<code>2</code>):","text":"<ul> <li>It specifies upsampling during detection.</li> <li>Upsampling makes the image larger temporarily, allowing the detector to find small faces.</li> <li>Trade-off:<ul> <li>Larger number \u2192 more accuracy</li> <li>But slower processing</li> </ul> </li> </ul> <p>Common values:</p> <ul> <li><code>0</code>: Fastest, but misses small faces</li> <li><code>1</code>: Better accuracy</li> <li><code>2</code>: Even better (your example)</li> <li><code>3</code>: Rarely used (slow)</li> </ul>"},{"location":"dlib/02-face-detection/#25-drawing-bounding-boxes","title":"2.5 Drawing Bounding Boxes","text":"<pre><code>for face in faces:\n    x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n    cv2.rectangle(img, (x1, y1), (x2, y2), (0,255,0), 2)\n</code></pre>"},{"location":"dlib/02-face-detection/#whats-happening","title":"What\u2019s happening:","text":"<ul> <li>Each <code>face</code> is a rectangle with:<ul> <li><code>.left()</code> = x coordinate of the left boundary</li> <li><code>.top()</code> = top boundary</li> <li><code>.right()</code> = right boundary</li> <li><code>.bottom()</code> = bottom boundary</li> </ul> </li> <li>We draw a green rectangle (<code>(0,255,0)</code>) of thickness <code>2</code> around each detected face.</li> </ul>"},{"location":"dlib/02-face-detection/#26-displaying-the-result","title":"2.6 Displaying the Result","text":"<pre><code>cv2.imshow(\"result\", img)\ncv2.waitKey(0)\n</code></pre> <ul> <li>Shows the image with the drawn rectangles</li> <li><code>waitKey(0)</code> waits indefinitely until the user closes the window or presses a key</li> </ul>"},{"location":"dlib/02-face-detection/#3-the-technology-behind-dlibs-face-detector","title":"3. The Technology Behind dlib\u2019s Face Detector","text":""},{"location":"dlib/02-face-detection/#31-what-is-hog-histogram-of-oriented-gradients","title":"3.1 What is HOG (Histogram of Oriented Gradients)?","text":"<p>HOG is a feature descriptor that captures edge directions in local regions of an image.</p> <p>Steps:</p> <ol> <li>Convert the image to grayscale</li> <li>Compute gradients</li> <li>Divide the image into small cells</li> <li>Build histograms of gradient orientations</li> <li>Normalize (improves robustness to lighting)</li> </ol> <p>Faces have distinctive gradient patterns (eyes, nose, chin), making HOG effective.</p>"},{"location":"dlib/02-face-detection/#32-svm-support-vector-machine","title":"3.2 SVM (Support Vector Machine)","text":"<p>dlib trains a linear SVM classifier:</p> <ul> <li>Positive samples \u2192 faces</li> <li>Negative samples \u2192 non-faces</li> <li>SVM learns a boundary to separate face-like patterns from everything else</li> </ul> <p>In detection:</p> <ul> <li>A sliding window scans the image</li> <li>HOG features extracted</li> <li>SVM classifies each window</li> <li>Windows classified as face are returned as bounding boxes</li> </ul>"},{"location":"dlib/02-face-detection/#33-why-use-upsampling","title":"3.3 Why Use Upsampling?","text":"<p>When a face is small (few pixels), HOG features are poor.</p> <p>Upsampling enlarges the image \u2192 improves detection.</p> <p>Example:</p> <ul> <li>original 100\u00d7100 \u2192 upsample \u00d72 \u2192 200\u00d7200</li> <li>small faces become easier to detect</li> </ul>"},{"location":"dlib/02-face-detection/#34-pros-and-cons-of-dlibs-hog-detector","title":"3.4 Pros and Cons of dlib's HOG Detector","text":""},{"location":"dlib/02-face-detection/#pros","title":"Pros","text":"<ul> <li>Fast on CPU</li> <li>No GPU required</li> <li>Works offline</li> <li>Very stable for frontal faces</li> <li>Lightweight, easy to integrate</li> </ul>"},{"location":"dlib/02-face-detection/#cons","title":"Cons","text":"<ul> <li>Not good for side faces or extreme angles</li> <li>Cannot detect very tiny faces well (even with upsampling)</li> <li>Slower than modern CNN detectors at high upsampling</li> </ul>"},{"location":"dlib/02-face-detection/#4-improvements-and-best-practices","title":"4. Improvements and Best Practices","text":""},{"location":"dlib/02-face-detection/#use-cnn-detector-for-higher-accuracy","title":"\u2714 Use CNN detector for higher accuracy","text":"<pre><code>cnn_detector = dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")\n</code></pre>"},{"location":"dlib/02-face-detection/#use-dlib-landmarks-for-face-alignment","title":"\u2714 Use dlib landmarks for face alignment","text":"<pre><code>predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n</code></pre>"},{"location":"dlib/02-face-detection/#use-opencv-to-resize-images-for-faster-performance","title":"\u2714 Use OpenCV to resize images for faster performance","text":"<pre><code>small = cv2.resize(img_rgb, (0,0), fx=0.5, fy=0.5)\n</code></pre>"},{"location":"dlib/02-face-detection/#5-summary","title":"5. Summary","text":"<p>This program:</p> <ol> <li>Loads dlib\u2019s HOG + SVM face detector</li> <li>Reads and converts an image from BGR \u2192 RGB</li> <li>Detects faces with upsampling</li> <li>Draws rectangles around detected faces</li> <li>Displays the result</li> </ol> <p>Behind the scenes, dlib uses:</p> <ul> <li>HOG to extract gradients</li> <li>SVM to classify face vs non-face</li> <li>Sliding window scanning</li> <li>Upsampling to find small faces</li> </ul> <p>This makes the script a clean and effective example of classical computer vision\u2013based face detection.</p>"},{"location":"dlib/02-face-detection/#face-detection-with-webcam","title":"Face Detection with Webcam","text":"<pre><code>import dlib  \nimport cv2  \n\nwebcam = cv2.VideoCapture(0)  \n\ndetector = dlib.get_frontal_face_detector()  \n\nwhile True:  \n    ret, frame = webcam.read()  \n    if not ret:  \n        print(\"End of webcam or failed to read frame.\")  \n\n    faces = detector(frame)  \n\n    for face in faces:  \n        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()  \n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  \n\n    cv2.imshow(\"result\", frame)  \n\n    if cv2.waitKey(60) == 27:  \n        print(\"Video playback interrupted by user.\")  \n        break  \n\nwebcam.release()  \ncv2.destroyAllWindows()\n</code></pre>"},{"location":"dlib/02-face-swap-in-dlib/","title":"\ud83d\udc0d Face Swap with Dlib and Python: A Step-by-Step Guide","text":"<p>Implementing a face swap using Dlib and Python is a classic Computer Vision project that combines Dlib's powerful facial landmark detection with OpenCV's image processing capabilities.</p> <p>The core principle is to align the geometric shape of the source face onto the target face and then seamlessly blend the resulting image.</p>"},{"location":"dlib/02-face-swap-in-dlib/#1-prerequisites-and-setup","title":"1. Prerequisites and Setup","text":"<p>You'll need to install the following Python libraries:</p> <ul> <li>Dlib: For accurate face detection and landmark prediction.</li> <li>OpenCV (<code>opencv-python</code>): For all image manipulation, warping, and blending tasks.</li> <li>NumPy: For efficient array and matrix operations.</li> </ul> <pre><code>pip install opencv-python dlib numpy\n</code></pre> <p>You must also download the Dlib pre-trained 68-point shape predictor model (usually named <code>shape_predictor_68_face_landmarks.dat</code>) and place it in your project directory.</p>"},{"location":"dlib/02-face-swap-in-dlib/#2-the-face-swap-algorithm","title":"2. The Face Swap Algorithm","text":"<p>The process involves four main stages:</p>"},{"location":"dlib/02-face-swap-in-dlib/#step-1-face-and-landmark-detection","title":"Step 1: Face and Landmark Detection","text":"<ul> <li>Tools: Dlib's <code>get_frontal_face_detector()</code> and <code>shape_predictor()</code>.</li> <li>Action: Detect the face rectangle in both the source image (the face you want to place) and the target image (the face you want to replace).</li> <li>Output: Extract the 68 facial landmarks (coordinates of the eyes, nose, mouth, and jawline) for both faces.</li> </ul>"},{"location":"dlib/02-face-swap-in-dlib/#step-2-geometric-alignment-and-triangulation","title":"Step 2: Geometric Alignment and Triangulation","text":"<p>To accurately warp the source face, we use a technique called Delaunay Triangulation on the landmarks:</p> <ol> <li>Delaunay Triangulation: The target face landmarks are used to partition the face area into a mesh of small, non-overlapping triangles.</li> <li>Corresponding Triangles: The same connectivity (topology) is applied to the source face landmarks, creating a corresponding set of triangles.</li> </ol>"},{"location":"dlib/02-face-swap-in-dlib/#step-3-affine-warping-the-warping-process","title":"Step 3: Affine Warping (The \"Warping\" Process)","text":"<p>This is where the actual face transfer happens:</p> <ol> <li>Iterate: Loop through every corresponding pair of triangles (one from the source face, one from the target face).</li> <li>Calculate Transformation: For each pair, calculate an Affine Transformation matrix using OpenCV's <code>cv2.getAffineTransform()</code>. This matrix describes the rotation, scaling, and shear needed to map the source triangle precisely onto the target triangle.</li> <li>Warp Pixels: Apply this matrix to the pixel data within the source face triangle using <code>cv2.warpAffine()</code>. This effectively warps the source face features (like an eye or a portion of the cheek) to align perfectly with the target face's geometry.</li> <li>Assemble: All the warped triangles are stitched together to create the new, aligned face, placed within the context of the target image.</li> </ol>"},{"location":"dlib/02-face-swap-in-dlib/#step-4-seamless-blending-the-final-touch","title":"Step 4: Seamless Blending (The Final Touch)","text":"<p>Directly pasting the warped face onto the target image would result in sharp, noticeable edges due to differences in lighting, skin tone, and color.</p> <ul> <li>Tool: OpenCV's Poisson Blending function, <code>cv2.seamlessClone()</code>.</li> <li>Action: This function is used to smoothly blend the boundary between the warped source face (the foreground) and the target image (the background).</li> <li>Result: It uses gradient-domain techniques to make the transition appear natural and realistic, ensuring the swapped face integrates seamlessly with the target image's lighting and background.</li> </ul>"},{"location":"dlib/02-face-swap-in-dlib/#summary-of-the-flow","title":"\ud83d\udca1 Summary of the Flow","text":"<p>The general flow of the code involves:</p> <ol> <li>Read Images and Detect Landmarks.</li> <li>Identify the Convex Hull (outer mask) of the target face.</li> <li>Perform Delaunay Triangulation on the landmark points.</li> <li>Calculate the transformation for each triangle.</li> <li>Warp the source face pixels based on these transformations.</li> <li>Use <code>cv2.seamlessClone()</code> to blend the final warped face onto the target image.</li> </ol>"},{"location":"dlib/03-cam_68/","title":"03 cam 68","text":"<pre><code>import dlib    \nimport cv2    \n\nwebcam = cv2.VideoCapture(0)    \n\ndetector = dlib.get_frontal_face_detector()  \npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  \n\nwhile True:    \n    ret, frame = webcam.read()  \n    if not ret:    \n        print(\"End of webcam or failed to read frame.\")    \n\n    faces = detector(frame)  \n    for face in faces:  \n        landmarks = predictor(frame, face)  \n\n        for n in range(68):  \n            x = landmarks.part(n).x  \n            y = landmarks.part(n).y  \n            cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)  \n            font = cv2.FONT_HERSHEY_SIMPLEX  \n            cv2.putText(frame, str(n), (x, y), font, 0.5, (0, 255, 0), 2)  \n\n    cv2.imshow(\"Webcam 68\", frame)  \n\n    if cv2.waitKey(60) == 27:    \n        print(\"Video playback interrupted by user.\")    \n        break    \nwebcam.release()    \ncv2.destroyAllWindows()\n</code></pre>"},{"location":"dlib/04-real-time_drowsiness_detection/","title":"Real-time Drowsiness Detection","text":""},{"location":"dlib/04-real-time_drowsiness_detection/#v1","title":"v1","text":"<pre><code>import dlib  \nimport cv2  \nimport numpy as np  \n\n# ---------- EAR computation ----------  \ndef compute_EAR(landmarks, eye_points):  \n    # A = |p2 - p6|  \n    A = np.linalg.norm( np.array([landmarks.part(eye_points[1]).x, landmarks.part(eye_points[1]).y]) -  \n                        np.array([landmarks.part(eye_points[5]).x, landmarks.part(eye_points[5]).y]) )  \n\n    # B = |p3 - p5|  \n    B = np.linalg.norm( np.array([landmarks.part(eye_points[2]).x, landmarks.part(eye_points[2]).y]) -  \n                        np.array([landmarks.part(eye_points[4]).x, landmarks.part(eye_points[4]).y]) )  \n\n    # C = |p1 - p4|  \n    C = np.linalg.norm( np.array([landmarks.part(eye_points[0]).x, landmarks.part(eye_points[0]).y]) -  \n                        np.array([landmarks.part(eye_points[3]).x, landmarks.part(eye_points[3]).y]) )  \n\n    EAR = ((A + B) / 2.0) / C  \n    return EAR  \n\n\n# ---------- dlib init ----------  \nwebcam = cv2.VideoCapture(0)  \ndetector = dlib.get_frontal_face_detector()  \npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  \n\n# left eye landmark index (dlib 68-model)  \nLEFT_EYE = [36, 37, 38, 39, 40, 41]  \nRIGHT_EYE = [42, 43, 44, 45, 46, 47]  \n\nEAR_THRESHOLD = 0.3        # EAR below this \u2192 closed eyes  \nCONSEC_FRAMES = 15         # how many frames of closure \u2192 fatigue alarm  \n\ncounter = 0                # closed-eye frame count  \n\nwhile True:  \n    ret, frame = webcam.read()  \n    if not ret:  \n        break  \n\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  \n    faces = detector(gray)  \n\n    for face in faces:  \n        landmarks = predictor(gray, face)  \n\n        # Compute EAR for both eyes  \n        leftEAR = compute_EAR(landmarks, LEFT_EYE)  \n        rightEAR = compute_EAR(landmarks, RIGHT_EYE)  \n        EAR = (leftEAR + rightEAR) / 2.0  \n\n        # Draw EAR text  \n        cv2.putText(frame, f\"EAR: {EAR:.3f}\", (30, 30),  \n                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)  \n\n        # ---- Fatigue detection ----  \n        if EAR &lt; EAR_THRESHOLD:  \n            counter += 1  \n            cv2.putText(frame, \"Eyes Closed\", (30, 70),  \n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)  \n\n            # Alarm when too many consecutive closed-eye frames  \n            if counter &gt;= CONSEC_FRAMES:  \n                cv2.putText(frame, \"!!! FATIGUE WARNING !!!\", (30, 120),  \n                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)  \n        else:  \n            counter = 0  \n            cv2.putText(frame, \"Eyes Open\", (30, 70),  \n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)  \n\n        # Draw 68 landmarks  \n        for n in range(68):  \n            x = landmarks.part(n).x  \n            y = landmarks.part(n).y  \n            cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)  \n            cv2.putText(frame, str(n), (x, y),  \n                        cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)  \n\n    cv2.imshow(\"Fatigue Monitor\", frame)  \n\n    if cv2.waitKey(1) == 27:     # ESC  \n        break  \n\nwebcam.release()  \ncv2.destroyAllWindows()\n</code></pre>"},{"location":"dlib/04-real-time_drowsiness_detection/#v2","title":"v2","text":"<pre><code>import numpy as np  \nimport dlib  \nimport cv2  \nfrom sklearn.metrics.pairwise import euclidean_distances  \nfrom PIL import Image, ImageDraw, ImageFont  \n\n\n# -----------------------------------------------  \n# Compute Eye Aspect Ratio (EAR)  \n# -----------------------------------------------  \ndef compute_ear(eye_points):  \n    \"\"\"  \n    Compute Eye Aspect Ratio (EAR) using 6 landmark points.    EAR = (||p2-p6|| + ||p3-p5||) / (2 * ||p1-p4||)  \n    Args:        eye_points (list): Six (x, y) tuples from facial landmarks.  \n    Returns:        float: EAR value.    \"\"\"    eye = np.array(eye_points)  \n\n    A = euclidean_distances([eye[1]], [eye[5]])[0][0]  \n    B = euclidean_distances([eye[2]], [eye[4]])[0][0]  \n    C = euclidean_distances([eye[0]], [eye[3]])[0][0]  \n\n    ear = (A + B) / (2.0 * C)  \n    return ear  \n\n\n# -----------------------------------------------  \n# Draw Chinese text on OpenCV image  \n# -----------------------------------------------  \ndef draw_chinese_text(img, text, position, text_color=(0, 255, 0), text_size=30):  \n    \"\"\"  \n    Draw Chinese text on a BGR OpenCV image using PIL.  \n    Args:        img (ndarray): BGR image.        text (str): Text to draw.        position (tuple): (x, y) position.        text_color (tuple): BGR color.        text_size (int): Font size.  \n    Returns:        ndarray: Updated BGR image.    \"\"\"    # Convert to PIL image    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  \n    drawer = ImageDraw.Draw(pil_img)  \n\n    # Load font (ensure `simhei.ttf` exists)  \n    font = ImageFont.truetype(\"fz-hei.ttf\", text_size, encoding=\"utf-8\")  \n\n    # Draw text  \n    drawer.text(position, text, fill=text_color, font=font)  \n\n    # Convert back to OpenCV BGR  \n    return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)  \n\n\n# -----------------------------------------------  \n# Draw the convex hull of the eye  \n# -----------------------------------------------  \ndef draw_eye_hull(img, eye_points):  \n    \"\"\"  \n    Draw convex hull of an eye region.  \n    Args:        img (ndarray): BGR frame.        eye_points (list): Eye landmark (6 points).    \"\"\"    hull = cv2.convexHull(np.array(eye_points))  \n    cv2.drawContours(img, [hull], -1, (0, 255, 0), 1)  \n\n\n# ==============================================================  \n# Main - Real-time Drowsiness Detection  \n# ==============================================================  \n\nEAR_THRESHOLD = 0.30        # EAR below this \u2192 possibly closed eyes  \nEAR_CONSEC_FRAMES = 50      # Number of consecutive frames before alert  \n\nframe_counter = 0  \n\ndetector = dlib.get_frontal_face_detector()  \npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  \n\ncap = cv2.VideoCapture(0)  \n\nwhile True:  \n    ret, frame = cap.read()  \n    if not ret:  \n        print(\"Failed to capture frame.\")  \n        break  \n\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  \n    faces = detector(gray)  \n\n    for face in faces:  \n        shape = predictor(gray, face)  \n        shape_np = np.array([[p.x, p.y] for p in shape.parts()])  \n\n        left_eye = shape_np[36:42]  \n        right_eye = shape_np[42:48]  \n\n        left_ear = compute_ear(left_eye)  \n        right_ear = compute_ear(right_eye)  \n        ear = (left_ear + right_ear) / 2.0  \n\n        # Eye drawing  \n        draw_eye_hull(frame, left_eye)  \n        draw_eye_hull(frame, right_eye)  \n\n        # Fatigue detection  \n        if ear &lt; EAR_THRESHOLD:  \n            frame_counter += 1  \n\n            if frame_counter &gt; EAR_CONSEC_FRAMES:  \n                frame = draw_chinese_text(frame, \"\uff01\uff01\uff01\u5371\u9669\uff01\uff01\uff01\", (250, 250), (0, 0, 255), 40)  \n        else:  \n            frame_counter = 0  \n\n        # EAR display  \n        info_text = f\"EAR: {ear:.2f}\"  \n        frame = draw_chinese_text(frame, info_text, (30, 30), (0, 255, 0), 28)  \n\n    cv2.imshow(\"Drowsiness Detection\", frame)  \n\n    if cv2.waitKey(1) == 27:  # ESC key  \n        break  \n\ncap.release()  \ncv2.destroyAllWindows()\n</code></pre>"},{"location":"dlib/Untitled/","title":"Untitled","text":"<pre><code>import numpy as np  \nimport dlib  \nimport cv2  \nfrom sklearn.metrics.pairwise import euclidean_distances  \nfrom PIL import Image, ImageDraw, ImageFont  \n\n\n# ------------------------------------------------------------  \n# Utility Functions  \n# ------------------------------------------------------------  \n\ndef mouth_aspect_ratio(shape):  \n    \"\"\"  \n    Compute MAR (Mouth Aspect Ratio).    Uses several vertical distances divided by the horizontal width.    \"\"\"    A = euclidean_distances(shape[50].reshape(1, 2), shape[58].reshape(1, 2))  \n    B = euclidean_distances(shape[51].reshape(1, 2), shape[57].reshape(1, 2))  \n    C = euclidean_distances(shape[52].reshape(1, 2), shape[56].reshape(1, 2))  \n    D = euclidean_distances(shape[48].reshape(1, 2), shape[54].reshape(1, 2))  \n\n    return ((A + B + C) / 3) / D  \n\n\ndef mouth_jaw_ratio(shape):  \n    \"\"\"  \n    Compute MJR (Mouth-Opening / Jaw Width Ratio).    Larger values indicate wider mouth opening.    \"\"\"    M = euclidean_distances(shape[48].reshape(1, 2), shape[54].reshape(1, 2))  # Mouth width  \n    J = euclidean_distances(shape[3].reshape(1, 2), shape[13].reshape(1, 2))   # Jaw width  \n    return M / J  \n\n\ndef put_chinese_text(img, text, position, color=(0, 255, 0), size=50):  \n    \"\"\"  \n    Draw Chinese text onto an OpenCV BGR image using PIL.    \"\"\"    if isinstance(img, np.ndarray):  \n        pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  \n    else:  \n        pil_img = img  \n\n    draw = ImageDraw.Draw(pil_img)  \n    font = ImageFont.truetype(\"fz-hei.ttf\", size, encoding=\"utf-8\")  \n\n    draw.text(position, text, color, font=font)  \n    return cv2.cvtColor(np.asarray(pil_img), cv2.COLOR_RGB2BGR)  \n\n\n# ------------------------------------------------------------  \n# Initialize Detector and Webcam  \n# ------------------------------------------------------------  \n\ndetector = dlib.get_frontal_face_detector()  \npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  \n\ncap = cv2.VideoCapture(0)  \n\n# ------------------------------------------------------------  \n# Main Loop  \n# ------------------------------------------------------------  \nwhile True:  \n    ret, frame = cap.read()  \n    if not ret:  \n        print(\"Failed to read frame.\")  \n        break  \n\n    faces = detector(frame, 0)  \n    for face in faces:  \n        # Extract 68 facial landmarks  \n        shape = predictor(frame, face)  \n        shape = np.array([[p.x, p.y] for p in shape.parts()])  \n\n        mar = mouth_aspect_ratio(shape)[0][0]  \n        mjr = mouth_jaw_ratio(shape)[0][0]  \n\n        result = \"\u6b63\u5e38\"  \n\n        if mar &gt; 0.5:  \n            result = \"\u5927\u7b11\"  \n        elif mjr &gt; 0.45:  \n            result = \"\u5fae\u7b11\"  \n\n        print(f\"MAR={mar:.3f}, MJR={mjr:.3f}, Result={result}\")  \n\n        frame = put_chinese_text(frame, result, (50, 100))  \n\n        mouth_hull = cv2.convexHull(shape[48:61])  \n        cv2.drawContours(frame, [mouth_hull], -1, (0, 255, 0), 1)  \n\n    cv2.imshow(\"Frame\", frame)  \n\n    if cv2.waitKey(1) == 27:  # ESC to exit  \n        break  \n\ncap.release()  \ncv2.destroyAllWindows()\n</code></pre>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/","title":"Face detection and landmark recognition with opencv and dlib","text":"<p>Below is a clean, professional, reference-ready English tutorial document introducing image-based and video-based face detection &amp; landmark recognition using OpenCV + dlib.  </p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#face-detection-and-landmark-recognition-with-opencv-and-dlib","title":"Face Detection and Landmark Recognition with OpenCV and dlib","text":"<p>A Practical Technical Guide for Image and Video Processing</p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#1-introduction","title":"1. Introduction","text":"<p>Face detection and facial landmark localization are foundational tasks in computer vision. They enable higher-level applications such as facial recognition, face alignment, emotion analysis, fatigue monitoring, and real-time human\u2013computer interaction.</p> <p>This document provides a complete, practical tutorial on using OpenCV and dlib to:</p> <ol> <li> <p>Detect human faces in static images</p> </li> <li> <p>Detect and track faces in video streams</p> </li> <li> <p>Extract 68-point facial landmarks</p> </li> </ol> <p>The content is designed to be:</p> <ul> <li> <p>Technically rigorous</p> </li> <li> <p>Easy to follow</p> </li> <li> <p>Suitable for long-term reference</p> </li> <li> <p>MacOS, Linux, and Windows compatible</p> </li> </ul>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#2-overview-of-the-pipeline","title":"2. Overview of the Pipeline","text":"<p>Both image and video face processing typically follow this pipeline:</p> <pre><code>Input \u2192 Face Detection \u2192 Landmark Extraction \u2192 Optional Post-processing\n</code></pre>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#21-face-detection","title":"2.1 Face Detection","text":"<p>OpenCV:</p> <ul> <li> <p>Haar Cascades (fast, outdated, less accurate)</p> </li> <li> <p>DNN-based detectors (more accurate, but heavy)</p> </li> </ul> <p>dlib:</p> <ul> <li> <p>HOG + SVM detector (fast CPU-only)</p> </li> <li> <p>CNN detector (high accuracy but slower)</p> </li> </ul> <p>For this guide, we use the widely available and CPU-friendly:</p> <p>\ud83d\udc49 dlib\u2019s HOG-based frontal face detector</p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#22-landmark-extraction","title":"2.2 Landmark Extraction","text":"<p>dlib provides:</p> <ul> <li> <p>A 68-point facial landmark model</p> </li> <li> <p>Pretrained file: <code>shape_predictor_68_face_landmarks.dat</code></p> </li> </ul> <p>It detects points around:</p> <ul> <li> <p>Jawline</p> </li> <li> <p>Eyebrows</p> </li> <li> <p>Eyes</p> </li> <li> <p>Nose</p> </li> <li> <p>Mouth</p> </li> </ul> <p>This guide uses the 68-point predictor.</p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#3-environment-setup","title":"3. Environment Setup","text":""},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#31-install-required-libraries","title":"3.1 Install Required Libraries","text":"<pre><code>pip install opencv-python dlib\n</code></pre> <p>Note: If dlib fails to install, you may need CMake and a compiler.</p> <ul> <li> <p>macOS (Homebrew): <code>brew install cmake</code></p> </li> <li> <p>Linux: <code>sudo apt install cmake g++</code></p> </li> <li> <p>Windows: install Build Tools for Visual Studio </p> </li> </ul>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#32-get-pretrained-models","title":"3.2 Get Pretrained Models","text":"<p>Download:</p> <ul> <li><code>shape_predictor_68_face_landmarks.dat</code></li> </ul> <p>Place it in your project directory.</p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#4-face-detection-in-images","title":"4. Face Detection in Images","text":""},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#41-full-example-code","title":"4.1 Full Example Code","text":"<pre><code>import cv2\nimport dlib\n\n# Load detector and predictor\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n\n# Read image\nimg = cv2.imread(\"face.jpg\")\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Detect faces\nfaces = detector(gray)\n\nfor face in faces:\n    x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    # Predict landmarks\n    shape = predictor(gray, face)\n\n    # Draw all 68 points\n    for i in range(68):\n        px, py = shape.part(i).x, shape.part(i).y\n        cv2.circle(img, (px, py), 2, (0, 255, 0), -1)\n        cv2.putText(img, str(i+1), (px-2, py-2), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)\n\n# Display\ncv2.imshow(\"Face Landmarks\", img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#42-notes-on-accuracy","title":"4.2 Notes on Accuracy","text":"<ul> <li> <p>Use clear, high-resolution images.</p> </li> <li> <p>Avoid heavy shadows or extreme angles.</p> </li> <li> <p>For side faces (profile), use dlib\u2019s CNN detector instead.</p> </li> </ul>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#5-real-time-face-detection-in-video","title":"5. Real-time Face Detection in Video","text":""},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#51-video-pipeline","title":"5.1 Video Pipeline","text":"<pre><code>Capture Frame \u2192 Convert to Gray \u2192 Detect Face \u2192 Extract Landmarks \u2192 Render \u2192 Loop\n</code></pre>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#52-example-code-webcam","title":"5.2 Example Code (Webcam)","text":"<pre><code>import cv2\nimport dlib\n\n# Setup\nwebcam = cv2.VideoCapture(0)\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n\nwhile True:\n    ret, frame = webcam.read()\n    if not ret:\n        print(\"Failed to capture frame.\")\n        break\n\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    faces = detector(gray)\n\n    for face in faces:\n        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        shape = predictor(gray, face)\n\n        # Draw landmarks\n        for i in range(68):\n            px, py = shape.part(i).x, shape.part(i).y\n            cv2.circle(frame, (px, py), 2, (0, 255, 0), -1)\n\n    cv2.imshow(\"Video Face Detection\", frame)\n\n    # Exit on ESC\n    if cv2.waitKey(1) == 27:\n        break\n\nwebcam.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#53-performance-tips","title":"5.3 Performance Tips","text":"<ul> <li> <p>Use a smaller resolution:</p> <pre><code>webcam.set(3, 640)\nwebcam.set(4, 480)\n</code></pre> </li> <li> <p>Use HOG detector for real-time CPU-only systems.</p> </li> <li> <p>For higher accuracy, switch to dlib CNN detector (requires GPU or strong CPU).</p> </li> </ul>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#6-landmark-index-reference-68-point-model","title":"6. Landmark Index Reference (68-point Model)","text":""},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#61-layout-summary","title":"6.1 Layout Summary","text":"Region Index Range Jawline 0\u201316 Eyebrows 17\u201326 Nose Bridge &amp; Bottom 27\u201335 Eyes 36\u201347 Mouth (outer + inner) 48\u201367"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#62-visualization","title":"6.2 Visualization","text":"<pre><code>    17---21    22---26\n  0---16       (eyes)\n     nose 27\u201335\n   mouth 48\u201367\n</code></pre> <p>This structure is widely used in:</p> <ul> <li> <p>Face alignment</p> </li> <li> <p>Eye aspect ratio (EAR) for fatigue detection</p> </li> <li> <p>Mouth opening detection (yawn/fatigue)</p> </li> <li> <p>Head pose estimation</p> </li> </ul>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#7-common-issues-solutions","title":"7. Common Issues &amp; Solutions","text":"Issue Cause Solution dlib installation fails Missing C++ tools Install CMake + compiler Detector finds no faces Low light, angle too extreme Add lighting, use CNN detector Video runs slowly Landmark computation too heavy Reduce resolution Facial landmarks jitter Detector re-runs each frame Use tracking (e.g., optical flow)"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#8-extending-the-system","title":"8. Extending the System","text":"<p>Once detection+landmarks work, you can build:</p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#face-recognition","title":"\u2713 Face Recognition","text":"<p>Use dlib's face embedding model (<code>dlib_face_recognition_resnet_model_v1.dat</code>).</p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#fatigue-detection","title":"\u2713 Fatigue Detection","text":"<p>Compute EAR (eye aspect ratio) + MAR (mouth aspect ratio).</p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#face-alignment","title":"\u2713 Face Alignment","text":"<p>Warp the face using eye/nose anchor points.</p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#emotion-recognition","title":"\u2713 Emotion Recognition","text":"<p>Feed aligned landmarks or image patches to ML models.</p>"},{"location":"dlib/face-detection-and-landmark-recognition-with-opencv-and-dlib/#9-conclusion","title":"9. Conclusion","text":"<p>This document provides a complete foundation for:</p> <ul> <li> <p>Face detection in images</p> </li> <li> <p>Real-time face detection in video streams</p> </li> <li> <p>Reliable 68-point landmark extraction</p> </li> <li> <p>Practical code that runs on all platforms</p> </li> </ul> <p>OpenCV + dlib remains one of the most accessible, transparent, and customizable solutions for classical facial analysis tasks.</p>"},{"location":"dlib/quicklinks/","title":"Quicklinks","text":"<ul> <li>https://github.com/davisking/dlib-models</li> </ul>"},{"location":"persons/feifei-li/","title":"Fei-Fei Li | \u674e\u98de\u98de","text":"<p>Fei-Fei Li is one of the most influential figures in modern artificial intelligence, particularly in the fields of computer vision and cognitive science. Widely recognized as a pioneer of the deep learning era, she is best known for initiating and leading the ImageNet project, a breakthrough that fundamentally transformed how machines perceive and understand visual data.</p> <p>Born in Beijing and raised in both China and the United States, Fei-Fei Li completed her undergraduate studies in physics at Princeton University and earned her Ph.D. in electrical engineering from the California Institute of Technology (Caltech). Her interdisciplinary background\u2014spanning physics, neuroscience, computer science, and engineering\u2014has played a key role in shaping her research vision: to build AI systems that see and think in ways inspired by human cognition.</p> <p>Fei-Fei Li is a Professor of Computer Science at Stanford University and the founding director of the Stanford Human-Centered AI Institute (HAI). She previously served as the director of the Stanford Artificial Intelligence Lab (SAIL) and was the Chief Scientist of AI/ML at Google Cloud, where she helped guide industrial AI research and deployment.</p> <p>Her most celebrated achievement, the ImageNet dataset, dramatically accelerated the progress of AI by providing an unprecedented large-scale benchmark for image recognition. It laid the groundwork for the success of deep convolutional neural networks and triggered the \u201cImageNet moment,\u201d a turning point that propelled modern AI into widespread real-world applications.</p> <p>Beyond technical contributions, Fei-Fei Li is also a leading advocate for human-centered AI, emphasizing fairness, ethics, inclusiveness, and the social responsibility of technology. She frequently engages with policymakers, educators, and global organizations to promote the safe and beneficial development of artificial intelligence.</p> <p>Today, Fei-Fei Li continues to shape the future of AI through her research in visual intelligence, robotics perception, and cognition-inspired machine learning, while also championing diversity and the human values at the core of technological advancement.</p>"},{"location":"persons/kaiming-he/","title":"Kaiming He | \u4f55\u51ef\u660e","text":"<p>Kaiming He is one of the most influential researchers in modern artificial intelligence, widely recognized for his groundbreaking contributions to deep learning, computer vision, and large-scale neural network training. His work has shaped the architectures and techniques that power today\u2019s most advanced AI systems.</p> <p>Born in China, Kaiming He received his bachelor\u2019s and master\u2019s degrees from Tsinghua University before earning his Ph.D. from the Chinese University of Hong Kong (CUHK). Early in his career, he joined Microsoft Research Asia (MSRA), where he conducted fundamental research in image processing and recognition. He later moved to Facebook AI Research (FAIR)\u2014now Meta AI\u2014where he became a leading figure in deep learning innovation. He is currently a researcher at FAIR and a professor at the University of Illinois Urbana-Champaign (UIUC), continuing to advance the frontiers of AI.</p> <p>Kaiming He is best known for being the first author of the ResNet (Deep Residual Learning) paper, introduced at CVPR 2016. ResNet pioneered the use of residual connections, solving the vanishing gradient problem and enabling the training of extremely deep neural networks. It remains one of the most cited and widely used architectures in the history of deep learning, forming the foundation for numerous modern models in computer vision, natural language processing, and multimodal AI.</p> <p>Beyond ResNet, Kaiming He has contributed to a wide range of influential works, including:</p> <ul> <li> <p>Mask R-CNN: A powerful framework for instance segmentation and object detection.</p> </li> <li> <p>Faster R-CNN: A landmark two-stage detector that greatly improved real-time object detection.</p> </li> <li> <p>R-CNN family: Foundational work that defined the modern era of object detection.</p> </li> <li> <p>FPN (Feature Pyramid Networks): A core technique enabling multi-scale feature learning.</p> </li> <li> <p>Momentum contrast (MoCo): A major contribution to self-supervised learning.</p> </li> </ul> <p>These projects have become standard components of modern computer vision systems, influencing both academic research and industrial applications such as autonomous driving, AR/VR, robotics, video analysis, and medical imaging.</p> <p>Kaiming He\u2019s research style is characterized by clarity, practicality, and strong mathematical intuition. His works often introduce simple yet transformative ideas that open new directions for the field. He has received numerous top honors in computer vision and AI, including multiple CVPR Best Paper Awards.</p> <p>Today, Kaiming He continues to explore fundamental problems in AI\u2014ranging from representation learning to model scalability\u2014while shaping the design principles of next-generation neural networks. His contributions have made him one of the most respected and impactful scientists in artificial intelligence.</p>"},{"location":"persons/persons/","title":"Persons","text":""},{"location":"persons/yangqing-jia/","title":"Yangqing Jia | \u8d3e\u626c\u9752","text":"<p>Yangqing Jia is a highly influential engineer and researcher in the field of artificial intelligence, best known as the creator of Caffe, one of the earliest and most impactful deep learning frameworks. His work has played a critical role in the transition of deep learning from academic research into practical, scalable industrial systems.</p> <p>Yangqing Jia was born and raised in China and later moved to the United States for higher education. He earned his Ph.D. in Computer Science from the University of California, Berkeley, where he studied under the supervision of Professor Trevor Darrell. During his doctoral research at Berkeley AI Research (BAIR), he developed Caffe, a groundbreaking deep learning framework that combined clarity, speed, and modularity. Released in 2014, Caffe quickly became one of the most widely used open-source frameworks in computer vision, powering research in image classification, detection, and multimedia applications.</p> <p>After his graduate work, Yangqing Jia joined Google, where he contributed to large-scale machine learning and deep learning infrastructure. He later moved to Facebook (now Meta), where he played a pivotal role in building PyTorch 2.0\u2019s underlying compiler technologies and advancing Facebook\u2019s AI platforms. At Facebook AI Research (FAIR), he also helped lead the development of Caffe2, the next-generation production-oriented deep learning framework designed for mobile deployment, distributed training, and cloud-scale inference.</p> <p>Beyond framework development, Yangqing Jia has been deeply involved in the engineering of large-scale AI platforms and the integration of deep learning into real-world products. His teams have worked on performance optimization, model deployment, machine learning compilers, and unified training/inference ecosystems that support billions of users worldwide.</p> <p>In industry, Yangqing Jia is respected for his ability to bridge research innovation with practical engineering, creating tools that are both theoretically strong and highly usable at scale. His open-source contributions\u2014particularly Caffe\u2014played a major role in democratizing deep learning and accelerating its wider adoption before PyTorch and TensorFlow became dominant.</p> <p>Following his contributions at Meta, he took on leadership roles in large technology companies, focusing on cloud-scale AI, machine learning infrastructure, and the modernization of software ecosystems for AI-driven applications.</p> <p>Today, Yangqing Jia continues to influence the direction of AI engineering, distributed training, and large-scale model deployment. His vision centers on building efficient, accessible, and robust AI systems that empower researchers and engineers worldwide. As both a researcher and an engineering leader, he remains one of the key figures in shaping how modern AI frameworks and infrastructures are designed.</p>"}]}